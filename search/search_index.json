{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to PROTOplast's documentation!","text":""},{"location":"#contents","title":"Contents","text":"<ul> <li>Readme</li> <li>Installation</li> <li>Tutorials</li> <li>Logging</li> <li>API<ul> <li>Single Cell Training</li> </ul> </li> <li>Contributing</li> <li>History</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Send your PR to the maintainer and we will review it make sure to write unit test for your PR and update the documentation</p>"},{"location":"history/","title":"History","text":""},{"location":"history/#011-2025-09-29","title":"0.1.1 (2025-09-29)","text":"<ul> <li>First release on PyPI.</li> <li>Early access first version</li> </ul>"},{"location":"history/#012-2025-10-27","title":"0.1.2 (2025-10-27)","text":"<ul> <li>Release bug fixes for Early access version</li> </ul>"},{"location":"history/#013-2025-11-16","title":"0.1.3 (2025-11-16)","text":"<ul> <li>Added inferencing api</li> <li>Fix relative path bug</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install protoplast, run this command in your terminal:</p> <pre><code>uv add protoplast\n</code></pre> <p>Or if you prefer to use <code>pip</code>:</p> <pre><code>pip install protoplast\n</code></pre>"},{"location":"installation/#from-source","title":"From source","text":"<p>The source files for protoplast can be downloaded from the Github repo.</p> <p>You can either clone the public repository:</p> <pre><code>git clone git://github.com/dataxight/protoplast\n</code></pre> <p>Or download the tarball:</p> <pre><code>curl -OJL https://github.com/dataxight/protoplast/tarball/master\n</code></pre> <p>Once you have a copy of the source, you can install it with:</p> <pre><code>cd protoplast\nuv pip install .\n</code></pre>"},{"location":"logging/","title":"Logging","text":"<p>There is two ways to set the logging level you can either import the <code>setup_console_logging</code> from <code>protoplast.utils</code> or if you import the trainer module this is automatically call for you and the logging level will be pass to each Ray processes.</p> <p>You can configure the log level via the <code>LOG_LEVEL</code> environment variable by default the level of <code>INFO</code> is used. This link show the supported level https://docs.python.org/3/library/logging.html#logging-levels</p>"},{"location":"readme/","title":"PROTOplast","text":"<p>Early developer preview of PROTOplast targets acceleration of ML model training workflows</p> <ul> <li>PyPI package: https://pypi.org/project/protoplast/</li> </ul>"},{"location":"readme/#features","title":"Features","text":"<ul> <li>Stream directly from remote/cloud storage (via runtime patching of anndata to use fsspec)</li> <li>Accelerated training of your ML models (14.5minutes on an A100 instance with 4 GPUs). Scale to multi-node clusters with zero code changes (with native Ray integration)</li> <li>Drop-in replacement of your custom ML training (by subclassing Lightning's LightningModule)</li> </ul>"},{"location":"readme/#getting-started","title":"Getting started","text":"<p>It's easy to get started with PROTOplast</p> <pre><code>from protoplast import RayTrainRunner, DistributedCellLineAnnDataset, LinearClassifier\nimport glob\n\nfiles = glob.glob(\"/data/tahoe100/*.h5ad\")\n\ntrainer = RayTrainRunner(\n   LinearClassifier,  # replace with your own model\n   DistributedCellLineAnnDataset,  # replace with your own Dataset\n   [\"num_genes\", \"num_classes\"],  # change according to what you need for your model\n)\ntrainer.train(file_paths=files)\n</code></pre> <p>Additional tutorials are available at https://protoplast.dataxight.com/tutorials</p> <p>Full documentation at https://protoplast.dataxight.com</p>"},{"location":"apis/","title":"API References","text":"<p>PROTOplast includes many different modules refer to the module related to your usage</p>"},{"location":"apis/#modules","title":"Modules","text":"<ul> <li>Single Cell Training</li> </ul>"},{"location":"apis/scrna/","title":"SCRNA API References","text":""},{"location":"apis/scrna/#trainer","title":"Trainer","text":""},{"location":"apis/scrna/#protoplast.scrna.anndata.trainer.RayTrainRunner","title":"<code>protoplast.scrna.anndata.trainer.RayTrainRunner</code>","text":"<p>A class to initialize the training this class automatically initializes Ray cluster or detect whether an existing cluster exist if there is an existing cluster it will automatically connect to it refer to <code>ray.init()</code> behavior</p> <p>Parameters:</p> Name Type Description Default <code>Model</code> <code>type[LightningModule]</code> <p>PyTorch Lightning model class</p> required <code>Ds</code> <code>type[DistributedAnnDataset]</code> <p>DistributedAnnDataset class</p> required <code>model_keys</code> <code>list[str]</code> <p>Keys to pass to model from <code>metadata_cb</code></p> required <code>metadata_cb</code> <code>Callable[[AnnData, dict], None]</code> <p>Callback to mutate metadata recommended for passing data from <code>obs</code> or <code>var</code> or any additional data your models required by default cell_line_metadata_cb</p> <code>cell_line_metadata_cb</code> <code>before_dense_cb</code> <code>Callable[[Tensor, str | int], Tensor]</code> <p>Callback to perform before densification of sparse matrix where the data at this point is still a sparse CSR Tensor, by default None</p> <code>None</code> <code>after_dense_cb</code> <code>Callable[[Tensor, str | int], Tensor]</code> <p>Callback to perform after densification of sparse matrix where the data at this point is a dense Tensor, by default None</p> <code>None</code> <code>shuffle_strategy</code> <code>ShuffleStrategy</code> <p>Strategy to split or randomize the data during the training, by default SequentialShuffleStrategy</p> <code>SequentialShuffleStrategy</code> <code>runtime_env_config</code> <code>dict | None</code> <p>These env config is to pass the RayTrainer processes, by default None</p> <code>None</code> <code>address</code> <code>str | None</code> <p>Override ray address, by default None</p> <code>None</code> <code>ray_trainer_strategy</code> <code>Strategy | None</code> <p>Override Ray Trainer Strategy if this is None it will default to RayDDP, by default None</p> <code>None</code> <code>sparse_key</code> <code>str</code> <p>description, by default \"X\",</p> <code>'X'</code> <p>Returns:</p> Type Description <code>RayTrainRunner</code> <p>Use this class to start the training</p> Source code in <code>src/protoplast/scrna/anndata/trainer.py</code> <pre><code>class RayTrainRunner:\n    \"\"\"A class to initialize the training this class automatically initializes Ray cluster or\n    detect whether an existing cluster exist if there is an existing cluster it will automatically\n    connect to it refer to `ray.init()` behavior\n\n    Parameters\n    ----------\n    Model : type[pl.LightningModule]\n        PyTorch Lightning model class\n    Ds : type[DistributedAnnDataset]\n        DistributedAnnDataset class\n    model_keys : list[str]\n        Keys to pass to model from `metadata_cb`\n    metadata_cb : Callable[[anndata.AnnData, dict], None], optional\n        Callback to mutate metadata recommended for passing data from `obs` or `var`\n        or any additional data your models required\n        by default cell_line_metadata_cb\n    before_dense_cb : Callable[[torch.Tensor, str  |  int], torch.Tensor], optional\n        Callback to perform before densification of sparse matrix where the data at this point\n        is still a sparse CSR Tensor, by default None\n    after_dense_cb : Callable[[torch.Tensor, str  |  int], torch.Tensor], optional\n        Callback to perform after densification of sparse matrix where the data at this point\n        is a dense Tensor, by default None\n    shuffle_strategy : ShuffleStrategy, optional\n        Strategy to split or randomize the data during the training, by default SequentialShuffleStrategy\n    runtime_env_config : dict | None, optional\n        These env config is to pass the RayTrainer processes, by default None\n    address : str | None, optional\n        Override ray address, by default None\n    ray_trainer_strategy : Strategy | None, optional\n        Override Ray Trainer Strategy if this is None it will default to RayDDP, by default None\n    sparse_key : str, optional\n        _description_, by default \"X\",\n    Returns\n    -------\n    RayTrainRunner\n        Use this class to start the training\n\n    \"\"\"\n\n    @beartype\n    def __init__(\n        self,\n        Model: type[pl.LightningModule],\n        Ds: type[DistributedAnnDataset],\n        model_keys: list[str],\n        metadata_cb: Callable[[anndata.AnnData, dict], None] = cell_line_metadata_cb,\n        before_dense_cb: Callable[[torch.Tensor, str | int], torch.Tensor] = None,\n        after_dense_cb: Callable[[torch.Tensor, str | int], torch.Tensor] = None,\n        shuffle_strategy: ShuffleStrategy = SequentialShuffleStrategy,\n        runtime_env_config: dict | None = None,\n        address: str | None = None,\n        ray_trainer_strategy: Strategy | None = None,\n        sparse_key: str = \"X\",\n    ):\n        self.Model = Model\n        self.Ds = Ds\n        self.model_keys = model_keys\n        self.metadata_cb = metadata_cb\n        self.shuffle_strategy = shuffle_strategy\n        self.sparse_key = sparse_key\n        self.before_dense_cb = before_dense_cb\n        self.after_dense_cb = after_dense_cb\n        if not ray_trainer_strategy:\n            self.ray_trainer_strategy = ray.train.lightning.RayDDPStrategy()\n        else:\n            self.ray_trainer_strategy = ray_trainer_strategy\n\n        # Init ray cluster\n        DEFAULT_RUNTIME_ENV_CONFIG = {\n            \"LOG_LEVEL\": os.getenv(\"LOG_LEVEL\", \"INFO\"),\n        }\n        if runtime_env_config is None:\n            runtime_env_config = DEFAULT_RUNTIME_ENV_CONFIG\n        ray.init(\n            address=address, runtime_env={**DEFAULT_RUNTIME_ENV_CONFIG, **runtime_env_config}, ignore_reinit_error=True\n        )\n\n        self.resources = ray.cluster_resources()\n\n    def _worker_fn(self):\n        warnings.filterwarnings(action=\"ignore\", module=\"ray\", category=DeprecationWarning)\n        Model, Ds, model_keys = self.Model, self.Ds, self.model_keys\n\n        def worker_fn(config):\n            ctx = ray.train.get_context()\n            if ctx:\n                rank = ctx.get_world_rank()\n            else:\n                rank = 0\n            indices = config.get(\"indices\")\n            ckpt_path = config.get(\"ckpt_path\")\n            scratch_path = config.get(\"scratch_path\")\n            scratch_content = config.get(\"scratch_content\")\n            logger.debug(\"Verifying storage path on worker node\")\n            try:\n                file = get_fsspec(scratch_path, \"r\")\n                read_content = file.read()\n                file.close()\n            except Exception as e:\n                logger.error(\"Failed to access shared storage path: %s\", scratch_path, exc_info=True)\n                raise Exception(\"Cannot access the shared storage. Please check your storage path.\") from e\n            if scratch_content != read_content:\n                logger.critical(\n                    f\"Content mismatch detected for path: {scratch_path}.Worker cannot read expected head node content.\"\n                )\n                raise Exception(\"Content mismatch detected. Please check your shared storage setup.\")\n            num_threads = int(os.environ.get(\"OMP_NUM_THREADS\", os.cpu_count()))\n            logger.debug(f\"=========Starting the training on {rank} with num threads: {num_threads}=========\")\n            model_params = indices.metadata\n            shuffle_strategy = config.get(\"shuffle_strategy\")\n            ann_dm = AnnDataModule(\n                indices,\n                Ds,\n                self.prefetch_factor,\n                self.sparse_key,\n                shuffle_strategy,\n                self.before_dense_cb,\n                self.after_dense_cb,\n                random_seed=config[\"random_seed\"],\n                **self.kwargs,\n            )\n            if model_keys:\n                model_params = {k: v for k, v in model_params.items() if k in model_keys}\n            model = Model(**model_params)\n            trainer = pl.Trainer(\n                max_epochs=self.max_epochs,\n                devices=\"auto\",\n                accelerator=_get_accelerator(),\n                strategy=self.ray_trainer_strategy,\n                plugins=[ray.train.lightning.RayLightningEnvironment()],\n                callbacks=[ray.train.lightning.RayTrainReportCallback()],\n                enable_checkpointing=True,\n                enable_progress_bar=config.get(\"enable_progress_bar\", True),\n            )\n            trainer = ray.train.lightning.prepare_trainer(trainer)\n            if config.get(\"worker_mode\") == \"inference\":\n                logger.debug(\"Starting inference mode\")\n                writer_cb = DistributedPredictionWriter(\n                    output_dir=self.result_storage_path, rank=rank, format=config[\"prediction_format\"]\n                )\n                trainer.callbacks.append(writer_cb)\n                trainer.predict(model, datamodule=ann_dm, ckpt_path=ckpt_path)\n            else:\n                logger.debug(\"Starting training mode\")\n                trainer.fit(model, datamodule=ann_dm, ckpt_path=ckpt_path)\n\n        return worker_fn\n\n    def _setup(\n        self,\n        file_paths: list[str],\n        batch_size: int,\n        test_size: float,\n        val_size: float,\n        prefetch_factor: int,\n        max_epochs: int,\n        thread_per_worker: int | None,\n        num_workers: int | None,\n        result_storage_path: str,\n        # read more here: https://lightning.ai/docs/pytorch/stable/common/trainer.html#fit\n        ckpt_path: str | None,\n        is_gpu: bool,\n        random_seed: int | None,\n        resource_per_worker: dict | None,\n        is_shuffled: bool,\n        enable_progress_bar: bool,\n        worker_mode: Literal[\"train\", \"inference\"],\n        **kwargs,\n    ):\n        self.result_storage_path = resolve_path_or_url(result_storage_path)\n        self.prefetch_factor = prefetch_factor\n        self.max_epochs = max_epochs\n        self.kwargs = kwargs\n        self.enable_progress_bar = enable_progress_bar\n        if not resource_per_worker:\n            if not thread_per_worker:\n                logger.info(\"Setting thread_per_worker to half of the available CPUs capped at 4\")\n                thread_per_worker = min(int((self.resources.get(\"CPU\", 2) - 1) / 2), 4)\n            resource_per_worker = {\"CPU\": thread_per_worker}\n        if is_gpu and self.resources.get(\"GPU\", 0) == 0:\n            logger.warning(\"`is_gpu = True` but there is no GPU found. Fallback to CPU.\")\n            is_gpu = False\n        if is_gpu:\n            if num_workers is None:\n                num_workers = int(self.resources.get(\"GPU\"))\n            scaling_config = ray.train.ScalingConfig(\n                num_workers=num_workers, use_gpu=True, resources_per_worker=resource_per_worker\n            )\n            resource_per_worker[\"GPU\"] = 1\n        else:\n            if num_workers is None:\n                num_workers = max(int((self.resources.get(\"CPU\", 2) - 1) / thread_per_worker), 1)\n            scaling_config = ray.train.ScalingConfig(\n                num_workers=num_workers, use_gpu=False, resources_per_worker=resource_per_worker\n            )\n        logger.info(f\"Using {num_workers} workers where each worker uses: {resource_per_worker}\")\n        start = time.time()\n\n        shuffle_strategy = self.shuffle_strategy(\n            [resolve_path_or_url(f) for f in file_paths],\n            batch_size,\n            num_workers * thread_per_worker,\n            test_size,\n            val_size,\n            random_seed,\n            metadata_cb=self.metadata_cb,\n            is_shuffled=is_shuffled,\n            **kwargs,\n        )\n        kwargs.pop(\"drop_last\", None)\n        kwargs.pop(\"pre_fetch_then_batch\", None)\n        indices = shuffle_strategy.split()\n        logger.debug(f\"Data splitting time: {time.time() - start:.2f} seconds\")\n        train_config = {\n            \"indices\": indices,\n            \"ckpt_path\": resolve_path_or_url(ckpt_path),\n            \"shuffle_strategy\": shuffle_strategy,\n            \"enable_progress_bar\": self.enable_progress_bar,\n            \"scratch_path\": os.path.join(self.result_storage_path, \"scratch.plt\"),\n            \"scratch_content\": str(uuid.uuid4()),\n            \"worker_mode\": worker_mode,\n            \"random_seed\": random_seed,\n        }\n        if worker_mode == \"inference\":\n            train_config[\"prediction_format\"] = kwargs[\"prediction_format\"]\n        par_trainer = ray.train.torch.TorchTrainer(\n            self._worker_fn(),\n            scaling_config=scaling_config,\n            train_loop_config=train_config,\n            run_config=ray.train.RunConfig(storage_path=self.result_storage_path),\n        )\n\n        logger.debug(\"Writing scratch content to share storage\")\n        scratch_path = train_config[\"scratch_path\"]\n        fs, path_on_fs = fsspec.core.url_to_fs(scratch_path)\n        parent_dir = os.path.dirname(path_on_fs)\n        if not fs.exists(parent_dir):\n            logger.debug(f\"Ensuring directory exists: {parent_dir}\")\n            fs.makedirs(parent_dir, exist_ok=True)\n        file = get_fsspec(scratch_path, mode=\"w\")\n        file.write(train_config[\"scratch_content\"])\n        file.close()\n        logger.debug(\"Spawning Ray worker and initiating distributed training\")\n        return par_trainer, indices\n\n    @beartype\n    def par_inference(\n        self,\n        file_paths: list[str],\n        ckpt_path: str | None = None,\n        result_storage_path: str = \"~/protoplast_results\",\n        batch_size: int = 2000,\n        prefetch_factor: int = 4,\n        thread_per_worker: int | None = None,\n        num_workers: int | None = None,\n        is_gpu: bool = True,\n        resource_per_worker: dict | None = None,\n        enable_progress_bar: bool = True,\n        prediction_format: Literal[\"csv\", \"parquet\"] = \"csv\",\n        **kwargs,\n    ):\n        \"\"\"Start parallel inference the order of the result is not guaranteed to be the same as input file\n\n        Parameters\n        ----------\n        file_paths : list[str]\n            List of h5ad AnnData files\n        batch_size : int, optional\n            How much data to fetch from disk, by default to 2000\n        prefetch_factor : int, optional\n            Total data fetch is prefetch_factor * batch_size, by default 4\n        thread_per_worker : int | None, optional\n            Amount of worker for each dataloader, by default None\n        num_workers : int | None, optional\n            Override number of Ray processes default to number of GPU(s) in the cluster, by default None\n        is_gpu : bool, optional\n            By default True turn this off if your system don't have any GPU, by default True\n        resource_per_worker : dict | None, optional\n            This get pass to Ray you can specify how much CPU or GPU each Ray process get, by default None\n        ckpt_path: str | None = None,\n            Path of the checkpoint if this is specified it will train from checkpoint otherwise it will start the\n            training from scratch, by default None\n        enable_progress_bar : bool\n            Whether to enable Trainer progress bar or not, by default True\n        Returns\n        -------\n        Result\n            The inference result from RayTrainer\n        \"\"\"\n        par_trainer, _ = self._setup(\n            file_paths,\n            batch_size,\n            0.0,\n            0.0,\n            prefetch_factor,\n            1,\n            thread_per_worker,\n            num_workers,\n            result_storage_path,\n            ckpt_path,\n            is_gpu,\n            None,\n            resource_per_worker,\n            False,\n            enable_progress_bar,\n            prediction_format=prediction_format,\n            worker_mode=\"inference\",\n            **kwargs,\n        )\n        # despite the confusing name we use fit to run inference here\n        result = par_trainer.fit()\n        # combine the result and order it correctly\n        return result\n\n    def inference(\n        self,\n        file_paths: list[str],\n        result_storage_path: str,\n        ckpt_path: str,\n        prediction_format: Literal[\"csv\", \"parquet\"] = \"csv\",\n        enable_progress_bar: bool = True,\n        batch_size=2000,\n    ):\n        \"\"\"Start inference in a single process order is guarantee to be the same as input file\n        don't use this in a distributed cluster\n        Parameters\n        ----------\n        file_paths : list[str]\n            List of h5ad AnnData files\n        result_storage_path : str\n            Path to store the prediction result\n        ckpt_path : str\n            Path of the checkpoint to run inference\n        enable_progress_bar : bool, optional\n            Whether to enable Trainer progress bar or not, by default True\n        batch_size : int, optional\n            How much data to fetch from disk, by default to 2000\n        \"\"\"\n        if sys.platform in (\"darwin\", \"win32\"):\n            override_thread = 0\n        else:\n            override_thread = 1\n        shuffle_strategy = self.shuffle_strategy(\n            [resolve_path_or_url(f) for f in file_paths],\n            batch_size,\n            override_thread,\n            0,\n            0,\n            None,\n            metadata_cb=self.metadata_cb,\n            is_shuffled=False,\n            prediction_format=prediction_format,\n        )\n        indices = shuffle_strategy.split()\n        writer_cb = PredictionWriterCallback(\n            output_path=resolve_path_or_url(result_storage_path), format=prediction_format\n        )\n        trainer = pl.Trainer(\n            devices=\"auto\",\n            accelerator=_get_accelerator(),\n            enable_progress_bar=enable_progress_bar,\n        )\n        trainer.callbacks.append(writer_cb)\n\n        ann_dm = AnnDataModule(\n            indices,\n            self.Ds,\n            4,\n            self.sparse_key,\n            SequentialShuffleStrategy,\n            self.before_dense_cb,\n            self.after_dense_cb,\n            batch_size=batch_size,\n            override_thread=override_thread,\n        )\n        model_params = indices.metadata\n        if self.model_keys:\n            model_params = {k: v for k, v in model_params.items() if k in self.model_keys}\n        model = self.Model(**model_params)\n        trainer.predict(model, datamodule=ann_dm, ckpt_path=resolve_path_or_url(ckpt_path))\n\n    @beartype\n    def train(\n        self,\n        file_paths: list[str],\n        batch_size: int = 2000,\n        test_size: float = 0.0,\n        val_size: float = 0.2,\n        prefetch_factor: int = 4,\n        max_epochs: int = 1,\n        thread_per_worker: int | None = None,\n        num_workers: int | None = None,\n        result_storage_path: str = \"~/protoplast_results\",\n        # read more here: https://lightning.ai/docs/pytorch/stable/common/trainer.html#fit\n        ckpt_path: str | None = None,\n        is_gpu: bool = True,\n        random_seed: int | None = 42,\n        resource_per_worker: dict | None = None,\n        is_shuffled: bool = False,\n        enable_progress_bar: bool = True,\n        **kwargs,\n    ):\n        \"\"\"Start the training\n\n        Parameters\n        ----------\n        file_paths : list[str]\n            List of h5ad AnnData files\n        batch_size : int, optional\n            How much data to fetch from disk, by default to 2000\n        test_size : float, optional\n            Fraction of test data for example 0.1 means 10% will be split for testing\n            default to 0.0\n        val_size : float, optional\n            Fraction of validation data for example 0.2 means 20% will be split for validation,\n            default to 0.2\n        prefetch_factor : int, optional\n            Total data fetch is prefetch_factor * batch_size, by default 4\n        max_epochs : int, optional\n            How many epoch(s) to train with, by default 1\n        thread_per_worker : int | None, optional\n            Amount of worker for each dataloader, by default None\n        num_workers : int | None, optional\n            Override number of Ray processes default to number of GPU(s) in the cluster, by default None\n        result_storage_path : str, optional\n            Path to store the loss, validation and checkpoint, by default \"~/protoplast_results\"\n        ckpt_path : str | None, optional\n            Path of the checkpoint if this is specified it will train from checkpoint otherwise it will start the\n            training from scratch, by default None\n        is_gpu : bool, optional\n            By default True turn this off if your system don't have any GPU, by default True\n        random_seed : int | None, optional\n            Set this to None for real training but for benchmarking and result replication\n            you can adjust the seed here, by default 42\n        resource_per_worker : dict | None, optional\n            This get pass to Ray you can specify how much CPU or GPU each Ray process get, by default None\n        enable_progress_bar : bool\n            Whether to enable Trainer progress bar or not, by default True\n        Returns\n        -------\n        Result\n            The training result from RayTrainer\n        \"\"\"\n        par_trainer, _ = self._setup(\n            file_paths,\n            batch_size,\n            test_size,\n            val_size,\n            prefetch_factor,\n            max_epochs,\n            thread_per_worker,\n            num_workers,\n            result_storage_path,\n            ckpt_path,\n            is_gpu,\n            random_seed,\n            resource_per_worker,\n            is_shuffled,\n            enable_progress_bar,\n            worker_mode=\"train\",\n            **kwargs,\n        )\n        return par_trainer.fit()\n</code></pre>"},{"location":"apis/scrna/#protoplast.scrna.anndata.trainer.RayTrainRunner.inference","title":"<code>inference(file_paths: list[str], result_storage_path: str, ckpt_path: str, prediction_format: Literal['csv', 'parquet'] = 'csv', enable_progress_bar: bool = True, batch_size=2000)</code>","text":"<p>Start inference in a single process order is guarantee to be the same as input file don't use this in a distributed cluster</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>list[str]</code> <p>List of h5ad AnnData files</p> required <code>result_storage_path</code> <code>str</code> <p>Path to store the prediction result</p> required <code>ckpt_path</code> <code>str</code> <p>Path of the checkpoint to run inference</p> required <code>enable_progress_bar</code> <code>bool</code> <p>Whether to enable Trainer progress bar or not, by default True</p> <code>True</code> <code>batch_size</code> <code>int</code> <p>How much data to fetch from disk, by default to 2000</p> <code>2000</code> Source code in <code>src/protoplast/scrna/anndata/trainer.py</code> <pre><code>def inference(\n    self,\n    file_paths: list[str],\n    result_storage_path: str,\n    ckpt_path: str,\n    prediction_format: Literal[\"csv\", \"parquet\"] = \"csv\",\n    enable_progress_bar: bool = True,\n    batch_size=2000,\n):\n    \"\"\"Start inference in a single process order is guarantee to be the same as input file\n    don't use this in a distributed cluster\n    Parameters\n    ----------\n    file_paths : list[str]\n        List of h5ad AnnData files\n    result_storage_path : str\n        Path to store the prediction result\n    ckpt_path : str\n        Path of the checkpoint to run inference\n    enable_progress_bar : bool, optional\n        Whether to enable Trainer progress bar or not, by default True\n    batch_size : int, optional\n        How much data to fetch from disk, by default to 2000\n    \"\"\"\n    if sys.platform in (\"darwin\", \"win32\"):\n        override_thread = 0\n    else:\n        override_thread = 1\n    shuffle_strategy = self.shuffle_strategy(\n        [resolve_path_or_url(f) for f in file_paths],\n        batch_size,\n        override_thread,\n        0,\n        0,\n        None,\n        metadata_cb=self.metadata_cb,\n        is_shuffled=False,\n        prediction_format=prediction_format,\n    )\n    indices = shuffle_strategy.split()\n    writer_cb = PredictionWriterCallback(\n        output_path=resolve_path_or_url(result_storage_path), format=prediction_format\n    )\n    trainer = pl.Trainer(\n        devices=\"auto\",\n        accelerator=_get_accelerator(),\n        enable_progress_bar=enable_progress_bar,\n    )\n    trainer.callbacks.append(writer_cb)\n\n    ann_dm = AnnDataModule(\n        indices,\n        self.Ds,\n        4,\n        self.sparse_key,\n        SequentialShuffleStrategy,\n        self.before_dense_cb,\n        self.after_dense_cb,\n        batch_size=batch_size,\n        override_thread=override_thread,\n    )\n    model_params = indices.metadata\n    if self.model_keys:\n        model_params = {k: v for k, v in model_params.items() if k in self.model_keys}\n    model = self.Model(**model_params)\n    trainer.predict(model, datamodule=ann_dm, ckpt_path=resolve_path_or_url(ckpt_path))\n</code></pre>"},{"location":"apis/scrna/#protoplast.scrna.anndata.trainer.RayTrainRunner.par_inference","title":"<code>par_inference(file_paths: list[str], ckpt_path: str | None = None, result_storage_path: str = '~/protoplast_results', batch_size: int = 2000, prefetch_factor: int = 4, thread_per_worker: int | None = None, num_workers: int | None = None, is_gpu: bool = True, resource_per_worker: dict | None = None, enable_progress_bar: bool = True, prediction_format: Literal['csv', 'parquet'] = 'csv', **kwargs)</code>","text":"<p>Start parallel inference the order of the result is not guaranteed to be the same as input file</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>list[str]</code> <p>List of h5ad AnnData files</p> required <code>batch_size</code> <code>int</code> <p>How much data to fetch from disk, by default to 2000</p> <code>2000</code> <code>prefetch_factor</code> <code>int</code> <p>Total data fetch is prefetch_factor * batch_size, by default 4</p> <code>4</code> <code>thread_per_worker</code> <code>int | None</code> <p>Amount of worker for each dataloader, by default None</p> <code>None</code> <code>num_workers</code> <code>int | None</code> <p>Override number of Ray processes default to number of GPU(s) in the cluster, by default None</p> <code>None</code> <code>is_gpu</code> <code>bool</code> <p>By default True turn this off if your system don't have any GPU, by default True</p> <code>True</code> <code>resource_per_worker</code> <code>dict | None</code> <p>This get pass to Ray you can specify how much CPU or GPU each Ray process get, by default None</p> <code>None</code> <code>ckpt_path</code> <code>str | None</code> <p>Path of the checkpoint if this is specified it will train from checkpoint otherwise it will start the training from scratch, by default None</p> <code>None</code> <code>enable_progress_bar</code> <code>bool</code> <p>Whether to enable Trainer progress bar or not, by default True</p> <code>True</code> <p>Returns:</p> Type Description <code>Result</code> <p>The inference result from RayTrainer</p> Source code in <code>src/protoplast/scrna/anndata/trainer.py</code> <pre><code>@beartype\ndef par_inference(\n    self,\n    file_paths: list[str],\n    ckpt_path: str | None = None,\n    result_storage_path: str = \"~/protoplast_results\",\n    batch_size: int = 2000,\n    prefetch_factor: int = 4,\n    thread_per_worker: int | None = None,\n    num_workers: int | None = None,\n    is_gpu: bool = True,\n    resource_per_worker: dict | None = None,\n    enable_progress_bar: bool = True,\n    prediction_format: Literal[\"csv\", \"parquet\"] = \"csv\",\n    **kwargs,\n):\n    \"\"\"Start parallel inference the order of the result is not guaranteed to be the same as input file\n\n    Parameters\n    ----------\n    file_paths : list[str]\n        List of h5ad AnnData files\n    batch_size : int, optional\n        How much data to fetch from disk, by default to 2000\n    prefetch_factor : int, optional\n        Total data fetch is prefetch_factor * batch_size, by default 4\n    thread_per_worker : int | None, optional\n        Amount of worker for each dataloader, by default None\n    num_workers : int | None, optional\n        Override number of Ray processes default to number of GPU(s) in the cluster, by default None\n    is_gpu : bool, optional\n        By default True turn this off if your system don't have any GPU, by default True\n    resource_per_worker : dict | None, optional\n        This get pass to Ray you can specify how much CPU or GPU each Ray process get, by default None\n    ckpt_path: str | None = None,\n        Path of the checkpoint if this is specified it will train from checkpoint otherwise it will start the\n        training from scratch, by default None\n    enable_progress_bar : bool\n        Whether to enable Trainer progress bar or not, by default True\n    Returns\n    -------\n    Result\n        The inference result from RayTrainer\n    \"\"\"\n    par_trainer, _ = self._setup(\n        file_paths,\n        batch_size,\n        0.0,\n        0.0,\n        prefetch_factor,\n        1,\n        thread_per_worker,\n        num_workers,\n        result_storage_path,\n        ckpt_path,\n        is_gpu,\n        None,\n        resource_per_worker,\n        False,\n        enable_progress_bar,\n        prediction_format=prediction_format,\n        worker_mode=\"inference\",\n        **kwargs,\n    )\n    # despite the confusing name we use fit to run inference here\n    result = par_trainer.fit()\n    # combine the result and order it correctly\n    return result\n</code></pre>"},{"location":"apis/scrna/#protoplast.scrna.anndata.trainer.RayTrainRunner.train","title":"<code>train(file_paths: list[str], batch_size: int = 2000, test_size: float = 0.0, val_size: float = 0.2, prefetch_factor: int = 4, max_epochs: int = 1, thread_per_worker: int | None = None, num_workers: int | None = None, result_storage_path: str = '~/protoplast_results', ckpt_path: str | None = None, is_gpu: bool = True, random_seed: int | None = 42, resource_per_worker: dict | None = None, is_shuffled: bool = False, enable_progress_bar: bool = True, **kwargs)</code>","text":"<p>Start the training</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>list[str]</code> <p>List of h5ad AnnData files</p> required <code>batch_size</code> <code>int</code> <p>How much data to fetch from disk, by default to 2000</p> <code>2000</code> <code>test_size</code> <code>float</code> <p>Fraction of test data for example 0.1 means 10% will be split for testing default to 0.0</p> <code>0.0</code> <code>val_size</code> <code>float</code> <p>Fraction of validation data for example 0.2 means 20% will be split for validation, default to 0.2</p> <code>0.2</code> <code>prefetch_factor</code> <code>int</code> <p>Total data fetch is prefetch_factor * batch_size, by default 4</p> <code>4</code> <code>max_epochs</code> <code>int</code> <p>How many epoch(s) to train with, by default 1</p> <code>1</code> <code>thread_per_worker</code> <code>int | None</code> <p>Amount of worker for each dataloader, by default None</p> <code>None</code> <code>num_workers</code> <code>int | None</code> <p>Override number of Ray processes default to number of GPU(s) in the cluster, by default None</p> <code>None</code> <code>result_storage_path</code> <code>str</code> <p>Path to store the loss, validation and checkpoint, by default \"~/protoplast_results\"</p> <code>'~/protoplast_results'</code> <code>ckpt_path</code> <code>str | None</code> <p>Path of the checkpoint if this is specified it will train from checkpoint otherwise it will start the training from scratch, by default None</p> <code>None</code> <code>is_gpu</code> <code>bool</code> <p>By default True turn this off if your system don't have any GPU, by default True</p> <code>True</code> <code>random_seed</code> <code>int | None</code> <p>Set this to None for real training but for benchmarking and result replication you can adjust the seed here, by default 42</p> <code>42</code> <code>resource_per_worker</code> <code>dict | None</code> <p>This get pass to Ray you can specify how much CPU or GPU each Ray process get, by default None</p> <code>None</code> <code>enable_progress_bar</code> <code>bool</code> <p>Whether to enable Trainer progress bar or not, by default True</p> <code>True</code> <p>Returns:</p> Type Description <code>Result</code> <p>The training result from RayTrainer</p> Source code in <code>src/protoplast/scrna/anndata/trainer.py</code> <pre><code>@beartype\ndef train(\n    self,\n    file_paths: list[str],\n    batch_size: int = 2000,\n    test_size: float = 0.0,\n    val_size: float = 0.2,\n    prefetch_factor: int = 4,\n    max_epochs: int = 1,\n    thread_per_worker: int | None = None,\n    num_workers: int | None = None,\n    result_storage_path: str = \"~/protoplast_results\",\n    # read more here: https://lightning.ai/docs/pytorch/stable/common/trainer.html#fit\n    ckpt_path: str | None = None,\n    is_gpu: bool = True,\n    random_seed: int | None = 42,\n    resource_per_worker: dict | None = None,\n    is_shuffled: bool = False,\n    enable_progress_bar: bool = True,\n    **kwargs,\n):\n    \"\"\"Start the training\n\n    Parameters\n    ----------\n    file_paths : list[str]\n        List of h5ad AnnData files\n    batch_size : int, optional\n        How much data to fetch from disk, by default to 2000\n    test_size : float, optional\n        Fraction of test data for example 0.1 means 10% will be split for testing\n        default to 0.0\n    val_size : float, optional\n        Fraction of validation data for example 0.2 means 20% will be split for validation,\n        default to 0.2\n    prefetch_factor : int, optional\n        Total data fetch is prefetch_factor * batch_size, by default 4\n    max_epochs : int, optional\n        How many epoch(s) to train with, by default 1\n    thread_per_worker : int | None, optional\n        Amount of worker for each dataloader, by default None\n    num_workers : int | None, optional\n        Override number of Ray processes default to number of GPU(s) in the cluster, by default None\n    result_storage_path : str, optional\n        Path to store the loss, validation and checkpoint, by default \"~/protoplast_results\"\n    ckpt_path : str | None, optional\n        Path of the checkpoint if this is specified it will train from checkpoint otherwise it will start the\n        training from scratch, by default None\n    is_gpu : bool, optional\n        By default True turn this off if your system don't have any GPU, by default True\n    random_seed : int | None, optional\n        Set this to None for real training but for benchmarking and result replication\n        you can adjust the seed here, by default 42\n    resource_per_worker : dict | None, optional\n        This get pass to Ray you can specify how much CPU or GPU each Ray process get, by default None\n    enable_progress_bar : bool\n        Whether to enable Trainer progress bar or not, by default True\n    Returns\n    -------\n    Result\n        The training result from RayTrainer\n    \"\"\"\n    par_trainer, _ = self._setup(\n        file_paths,\n        batch_size,\n        test_size,\n        val_size,\n        prefetch_factor,\n        max_epochs,\n        thread_per_worker,\n        num_workers,\n        result_storage_path,\n        ckpt_path,\n        is_gpu,\n        random_seed,\n        resource_per_worker,\n        is_shuffled,\n        enable_progress_bar,\n        worker_mode=\"train\",\n        **kwargs,\n    )\n    return par_trainer.fit()\n</code></pre>"},{"location":"apis/scrna/#datamodule","title":"DataModule","text":"<p>Wrapper around Dataset on how the data should be forward to the Lightning Model support hooks at various Lifecycle when the data get pass to the model</p>"},{"location":"apis/scrna/#protoplast.scrna.anndata.torch_dataloader.AnnDataModule","title":"<code>protoplast.scrna.anndata.torch_dataloader.AnnDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> Source code in <code>src/protoplast/scrna/anndata/torch_dataloader.py</code> <pre><code>class AnnDataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        indices: dict,\n        dataset: DistributedAnnDataset,\n        prefetch_factor: int,\n        sparse_key: str,\n        shuffle_strategy: ShuffleStrategy,\n        before_dense_cb: Callable[[torch.Tensor, str | int], torch.Tensor] = None,\n        after_dense_cb: Callable[[torch.Tensor, str | int], torch.Tensor] = None,\n        override_thread: int | None = None,\n        **kwargs,\n    ):\n        super().__init__()\n        self.indices = indices\n        self.dataset = dataset\n        if override_thread is not None:\n            num_threads = override_thread\n        else:\n            num_threads = int(os.environ.get(\"OMP_NUM_THREADS\", os.cpu_count()))\n        self.loader_config = dict(\n            num_workers=num_threads,\n        )\n        if num_threads &gt; 0:\n            self.loader_config[\"prefetch_factor\"] = prefetch_factor\n            self.loader_config[\"persistent_workers\"] = True\n        if shuffle_strategy.is_mixer():\n            self.loader_config[\"batch_size\"] = shuffle_strategy.mini_batch_size\n            self.loader_config[\"collate_fn\"] = shuffle_strategy.mixer\n            self.loader_config[\"drop_last\"] = True\n        else:\n            self.loader_config[\"batch_size\"] = None\n        self.sparse_key = sparse_key\n        self.before_dense_cb = before_dense_cb\n        self.after_dense_cb = after_dense_cb\n        self.kwargs = kwargs\n\n    def setup(self, stage):\n        # this is not necessary but it is here in case we want to download data to local node in the future\n        if stage == \"fit\":\n            self.train_ds = self.dataset.create_distributed_ds(self.indices, self.sparse_key, **self.kwargs)\n            self.val_ds = self.dataset.create_distributed_ds(self.indices, self.sparse_key, \"val\", **self.kwargs)\n        if stage == \"test\":\n            self.val_ds = self.dataset.create_distributed_ds(self.indices, self.sparse_key, \"test\", **self.kwargs)\n        if stage == \"predict\":\n            self.predict_ds = self.dataset.create_distributed_ds(self.indices, self.sparse_key, **self.kwargs)\n\n    def train_dataloader(self):\n        return DataLoader(self.train_ds, **self.loader_config)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_ds, **self.loader_config)\n\n    def test_dataloader(self):\n        # for now not support testing for splitting will support it soon in the future\n        return DataLoader(self.val_ds, **self.loader_config)\n\n    def predict_dataloader(self):\n        return DataLoader(self.predict_ds, **self.loader_config)\n\n    def densify(self, x, idx: str | int = None):\n        if isinstance(x, torch.Tensor):\n            if self.before_dense_cb:\n                x = self.before_dense_cb(x, idx)\n            if x.is_sparse or x.is_sparse_csr:\n                x = x.to_dense()\n            if self.after_dense_cb:\n                x = self.after_dense_cb(x, idx)\n        return x\n\n    def on_after_batch_transfer(self, batch, dataloader_idx):\n        if (type(batch) is list) or (type(batch) is tuple):\n            return [self.densify(d, i) for i, d in enumerate(batch)]\n        elif isinstance(batch, dict):\n            return {k: self.densify(v, k) for k, v in batch.items()}\n        elif isinstance(batch, torch.Tensor):\n            return self.densify(batch)\n        else:\n            return batch\n</code></pre>"},{"location":"apis/scrna/#dataset","title":"Dataset","text":"<p>For fetching and sending data to the model </p>"},{"location":"apis/scrna/#protoplast.scrna.anndata.torch_dataloader.DistributedAnnDataset","title":"<code>protoplast.scrna.anndata.torch_dataloader.DistributedAnnDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Dataset that support multiworker distribution this version will yield the data in a sequential manner</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>list[str]</code> <p>List of files</p> required <code>indices</code> <code>list[list[int]]</code> <p>List of indices from <code>SplitInfo</code></p> required <code>metadata</code> <code>dict</code> <p>Metadata dictionary for sending data to the model or other logical purposes</p> required <code>sparse_key</code> <code>str</code> <p>AnnData key for the sparse matrix usually it is \"X\" if \"layers\" please use the dot notation for example \"layers.attr\" where attr is the key in the layer you want to refer to</p> required <code>mini_batch_size</code> <code>int</code> <p>How many observation to send to the model must be less than <code>batch_size</code>, by default None and will send the whole batch if this is not specified</p> <code>None</code> Source code in <code>src/protoplast/scrna/anndata/torch_dataloader.py</code> <pre><code>class DistributedAnnDataset(torch.utils.data.IterableDataset):\n    \"\"\"Dataset that support multiworker distribution this version will yield the data\n    in a sequential manner\n\n    Parameters\n    ----------\n    file_paths : list[str]\n        List of files\n    indices : list[list[int]]\n        List of indices from `SplitInfo`\n    metadata : dict\n        Metadata dictionary for sending data to the model or other logical purposes\n    sparse_key : str\n        AnnData key for the sparse matrix usually it is \"X\" if \"layers\" please use the dot notation for example\n        \"layers.attr\" where attr is the key in the layer you want to refer to\n    mini_batch_size : int, optional\n        How many observation to send to the model must be less than `batch_size`, by default None\n        and will send the whole batch if this is not specified\n    \"\"\"\n\n    def __init__(\n        self,\n        file_paths: list[str],\n        indices: list[list[int]],\n        metadata: dict,\n        sparse_key: str,\n        mini_batch_size: int = None,\n        **kwargs,  # FIXME: workaround for PROTO-23\n    ):\n        # use first file as reference first\n        self.files = file_paths\n        self.sparse_key = sparse_key\n        self.X = None\n        self.ad = None\n        # map each gene to an index\n        for k, v in metadata.items():\n            setattr(self, k, v)\n        self.metadata = metadata\n        self.batches = indices\n        self.mini_batch_size = mini_batch_size\n        self.counter = 0\n        if \"random_seed\" in kwargs:\n            self.random_seed = kwargs[\"random_seed\"]\n        else:\n            self.random_seed = None\n\n    @classmethod\n    def create_distributed_ds(cls, indices: SplitInfo, sparse_key: str, mode: str = \"train\", **kwargs):\n        indices = indices.to_dict() if isinstance(indices, SplitInfo) else indices\n        return cls(\n            indices[\"files\"],\n            indices[f\"{mode}_indices\"],\n            indices[\"metadata\"],\n            sparse_key,\n            mini_batch_size=indices.get(\"mini_batch_size\"),\n            **kwargs,\n        )\n\n    def _init_rank(self):\n        worker_info = get_worker_info()\n        if worker_info is None:\n            self.wid = 0\n            self.nworkers = 1\n        else:\n            self.wid = worker_info.id\n            self.nworkers = worker_info.num_workers\n        try:\n            w_rank = td.get_rank()\n            w_size = td.get_world_size()\n        except ValueError:\n            w_rank = -1\n            w_size = -1\n        if w_rank &gt;= 0:\n            self.ray_rank = w_rank\n            self.ray_size = w_size\n        else:\n            self.ray_rank = 0\n            self.ray_size = 1\n        self.global_rank = self.ray_rank * self.nworkers + self.wid\n        self.total_workers = self.ray_size * self.nworkers\n\n    def _process_sparse(self, mat) -&gt; torch.Tensor:\n        if sp.issparse(mat):\n            return torch.sparse_csr_tensor(\n                torch.from_numpy(mat.indptr).long(),\n                torch.from_numpy(mat.indices).long(),\n                torch.from_numpy(mat.data).float(),\n                mat.shape,\n            )\n        return torch.from_numpy(mat).float()\n\n    def _get_mat_by_range(self, ad: anndata.AnnData, start: int, end: int) -&gt; sp.csr_matrix:\n        if self.sparse_key == \"X\":\n            return ad.X[start:end]\n        elif \"layers\" in self.sparse_key:\n            _, attr = self.sparse_key.split(\".\")\n            return ad.layers[attr][start:end]\n        else:\n            raise Exception(\"Sparse key not supported\")\n\n    def transform(self, start: int, end: int):\n        \"\"\"The subclass should implement the logic to get more data for the cell. It can leverage this super function\n        to efficiently get X as a sparse tensor. An example of how to get to more data from the cell is\n        `self.ad.obs[\"key\"][start:end]` where you must only fetch a subset of this data with `start` and `end`\n\n\n        Parameters\n        ----------\n        start : int\n            Starting index of this batch\n        end : int\n            Ending index of this batch\n\n        Returns\n        -------\n        Any\n            Usually a tensor, a list of tensor or dictionary with tensor value\n        \"\"\"\n        # by default we just return the matrix\n        # sometimes, the h5ad file stores X as the dense matrix,\n        # so we have to make sure it is a sparse matrix before returning\n        # the batch item\n        if self.X is None:\n            # we don't have the X upstream, so we have to incurr IO to fetch it\n            self.X = self._get_mat_by_range(self.ad, start, end)\n        X = self._process_sparse(self.X)\n        return X\n\n    def __len__(self):\n        try:\n            world_size = td.get_world_size()\n        except ValueError:\n            logging.warning(\"Not using tdd default to world size 1\")\n            world_size = 1\n        if self.mini_batch_size:\n            total_sample = sum(end - start for i in range(len(self.files)) for start, end in self.batches[i])\n            return math.ceil(total_sample / self.mini_batch_size / world_size)\n        return sum(1 for i in range(len(self.files)) for _ in self.batches[i]) / world_size\n\n    def __iter__(self):\n        self._init_rank()\n        if self.random_seed:\n            logger.debug(f\"Counter value: {self.counter}, seed value: {self.random_seed}\")\n            random.seed(self.random_seed + self.counter)\n        for fidx, f in enumerate(self.files):\n            self.ad = anndata.read_h5ad(f, backed=\"r\")\n            # ensure each epoch have different data order\n            random.shuffle(self.batches[fidx])\n            total_mini_batches = 0\n            if self.mini_batch_size is not None:\n                total_mini_batches = sum((end - start) // self.mini_batch_size for start, end in self.batches[fidx])\n            else:\n                # Treat whole batch as one mini-batch\n                self.mini_batch_size = self.batches[fidx][0][1] - self.batches[fidx][0][0]\n                total_mini_batches = len(self.batches[fidx])\n\n            # Find range of the batches assigned to this worker\n            mini_batch_per_worker = (\n                total_mini_batches // self.total_workers\n            )  # This number is ALWAYS divisble by total_workers\n            mini_batch_per_batch = (\n                self.batches[fidx][0][1] - self.batches[fidx][0][0]\n            ) // self.mini_batch_size  # Will be 1 if mini_batch_size is None\n            if mini_batch_per_batch == 0:\n                mini_batch_per_batch = 1  # Handle case when mini_batch_size &gt; batch size\n\n            start_mini_batch_gidx = self.global_rank * mini_batch_per_worker  # a.k.a offset\n            end_mini_batch_gidx = start_mini_batch_gidx + mini_batch_per_worker  # exclusive\n\n            start_batch_gidx = start_mini_batch_gidx // mini_batch_per_batch\n            end_batch_gidx = end_mini_batch_gidx // mini_batch_per_batch\n\n            # Adjust the index of the first and last mini-batch in the first and last batch respectively\n            # Only apply when a batch contains multiple mini-batches\n            current_worker_batches = self.batches[fidx][start_batch_gidx : end_batch_gidx + 1]\n            if mini_batch_per_batch != 1:\n                # Offset the index of first mini-batch\n                current_worker_batches[0] = (\n                    current_worker_batches[0][0]\n                    + (start_mini_batch_gidx % mini_batch_per_batch) * self.mini_batch_size,\n                    current_worker_batches[0][1],\n                )\n\n                if len(current_worker_batches) &gt; 1:\n                    # Offset the index of last mini-batch\n                    total_mini_batches_exclude_last = sum(\n                        (end - start) // self.mini_batch_size for start, end in current_worker_batches[:-1]\n                    )\n                    remainder = mini_batch_per_worker - total_mini_batches_exclude_last\n                    current_worker_batches[-1] = (\n                        current_worker_batches[-1][0],\n                        current_worker_batches[-1][0] + remainder * self.mini_batch_size,\n                    )\n\n            # NOTE: Black magic to improve read performance during data yielding\n            if len(current_worker_batches) &gt; 1:\n                current_worker_batches = current_worker_batches[1:] + current_worker_batches[:1]\n\n            yielded_mini_batches = 0\n            for i, (start, end) in enumerate(current_worker_batches):\n                # Fetch the whole block &amp; start yielding data\n                X = self._get_mat_by_range(self.ad, start, end)\n                for i in range(0, X.shape[0], self.mini_batch_size):\n                    # index on the X coordinates\n                    b_start, b_end = i, min(i + self.mini_batch_size, X.shape[0])\n\n                    # index on the adata coordinates\n                    global_start, global_end = start + i, min(start + i + self.mini_batch_size, end)\n                    self.X = X[b_start:b_end]\n\n                    yield self.transform(global_start, global_end)\n                    yielded_mini_batches += 1\n\n                    if yielded_mini_batches &gt;= mini_batch_per_worker:\n                        break\n\n                if yielded_mini_batches &gt;= mini_batch_per_worker:\n                    break\n        self.counter += 1\n</code></pre>"},{"location":"apis/scrna/#protoplast.scrna.anndata.torch_dataloader.DistributedAnnDataset.transform","title":"<code>transform(start: int, end: int)</code>","text":"<p>The subclass should implement the logic to get more data for the cell. It can leverage this super function to efficiently get X as a sparse tensor. An example of how to get to more data from the cell is <code>self.ad.obs[\"key\"][start:end]</code> where you must only fetch a subset of this data with <code>start</code> and <code>end</code></p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>Starting index of this batch</p> required <code>end</code> <code>int</code> <p>Ending index of this batch</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Usually a tensor, a list of tensor or dictionary with tensor value</p> Source code in <code>src/protoplast/scrna/anndata/torch_dataloader.py</code> <pre><code>def transform(self, start: int, end: int):\n    \"\"\"The subclass should implement the logic to get more data for the cell. It can leverage this super function\n    to efficiently get X as a sparse tensor. An example of how to get to more data from the cell is\n    `self.ad.obs[\"key\"][start:end]` where you must only fetch a subset of this data with `start` and `end`\n\n\n    Parameters\n    ----------\n    start : int\n        Starting index of this batch\n    end : int\n        Ending index of this batch\n\n    Returns\n    -------\n    Any\n        Usually a tensor, a list of tensor or dictionary with tensor value\n    \"\"\"\n    # by default we just return the matrix\n    # sometimes, the h5ad file stores X as the dense matrix,\n    # so we have to make sure it is a sparse matrix before returning\n    # the batch item\n    if self.X is None:\n        # we don't have the X upstream, so we have to incurr IO to fetch it\n        self.X = self._get_mat_by_range(self.ad, start, end)\n    X = self._process_sparse(self.X)\n    return X\n</code></pre>"},{"location":"apis/scrna/#protoplast.scrna.anndata.strategy.ShuffleStrategy","title":"<code>protoplast.scrna.anndata.strategy.ShuffleStrategy</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Strategy on how to data should be split and shuffle during the training</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>list[str]</code> <p>List of file paths</p> required <code>batch_size</code> <code>int</code> <p>How much data to fetch</p> required <code>total_workers</code> <code>int</code> <p>Total workers this is equal to number of processes times number of threads per process</p> required <code>test_size</code> <code>float | None</code> <p>Fraction of test data for example 0.1 means 10% will be split for testing, by default None</p> <code>None</code> <code>validation_size</code> <code>float | None</code> <p>Fraction of validation data for example 0.2 means 20% will be split for validation, by default None</p> <code>None</code> <code>random_seed</code> <code>int | None</code> <p>Seed to randomize the split set this to None if you want this to be completely random, by default 42</p> <code>42</code> <code>metadata_cb</code> <code>Callable[[AnnData, dict], None] | None</code> <p>Callback to mutate metadata recommended for passing data from <code>obs</code> or <code>var</code> or any additional data your models required by default cell_line_metadata_cb</p> <code>None</code> <code>is_shuffled</code> <code>bool</code> <p>Whether to shuffle the data or not this will be deprecated soon, by default True</p> <code>True</code> Source code in <code>src/protoplast/scrna/anndata/strategy.py</code> <pre><code>class ShuffleStrategy(ABC):\n    \"\"\"Strategy on how to data should be split and shuffle during\n    the training\n\n    Parameters\n    ----------\n    file_paths : list[str]\n        List of file paths\n    batch_size : int\n        How much data to fetch\n    total_workers : int\n        Total workers this is equal to number of processes times number of threads per process\n    test_size : float | None, optional\n        Fraction of test data for example 0.1 means 10% will be split for testing, by default None\n    validation_size : float | None, optional\n        Fraction of validation data for example 0.2 means 20% will be split for validation, by default None\n    random_seed : int | None, optional\n        Seed to randomize the split set this to None if you want this to be completely random, by default 42\n    metadata_cb : Callable[[anndata.AnnData, dict], None] | None, optional\n        Callback to mutate metadata recommended for passing data from `obs` or `var`\n        or any additional data your models required\n        by default cell_line_metadata_cb\n    is_shuffled : bool, optional\n        Whether to shuffle the data or not this will be deprecated soon, by default True\n    \"\"\"\n\n    def __init__(\n        self,\n        file_paths: list[str],\n        batch_size: int,\n        total_workers: int,\n        test_size: float | None = None,\n        validation_size: float | None = None,\n        random_seed: int | None = 42,\n        metadata_cb: Callable[[anndata.AnnData, dict], None] | None = None,\n        is_shuffled: bool = True,\n    ) -&gt; None:\n        self.file_paths = file_paths\n        self.batch_size = batch_size\n        self.total_workers = total_workers\n        self.test_size = test_size\n        self.validation_size = validation_size\n        self.random_seed = random_seed\n        self.metadata_cb = metadata_cb\n        self.is_shuffled = is_shuffled\n        self.rng = random.Random(random_seed) if random_seed else random.Random()\n\n    @staticmethod\n    def is_mixer():\n        return False\n\n    @abstractmethod\n    def split(self) -&gt; SplitInfo:\n        \"\"\"\n        How you want to split the data in each worker must return SplitInfo\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def mixer(self, batch: list) -&gt; any:\n        \"\"\"\n        If your Dataset only return 1 sample and not prebatched\n        this need to be implemented\n        \"\"\"\n        pass\n</code></pre>"},{"location":"apis/scrna/#protoplast.scrna.anndata.strategy.ShuffleStrategy.mixer","title":"<code>mixer(batch: list) -&gt; any</code>  <code>abstractmethod</code>","text":"<p>If your Dataset only return 1 sample and not prebatched this need to be implemented</p> Source code in <code>src/protoplast/scrna/anndata/strategy.py</code> <pre><code>@abstractmethod\ndef mixer(self, batch: list) -&gt; any:\n    \"\"\"\n    If your Dataset only return 1 sample and not prebatched\n    this need to be implemented\n    \"\"\"\n    pass\n</code></pre>"},{"location":"apis/scrna/#protoplast.scrna.anndata.strategy.ShuffleStrategy.split","title":"<code>split() -&gt; SplitInfo</code>  <code>abstractmethod</code>","text":"<p>How you want to split the data in each worker must return SplitInfo</p> Source code in <code>src/protoplast/scrna/anndata/strategy.py</code> <pre><code>@abstractmethod\ndef split(self) -&gt; SplitInfo:\n    \"\"\"\n    How you want to split the data in each worker must return SplitInfo\n    \"\"\"\n    pass\n</code></pre>"},{"location":"apis/scrna/#protoplast.scrna.anndata.strategy.SplitInfo","title":"<code>protoplast.scrna.anndata.strategy.SplitInfo</code>  <code>dataclass</code>","text":"Source code in <code>src/protoplast/scrna/anndata/strategy.py</code> <pre><code>@dataclass\nclass SplitInfo:\n    files: list[str]\n    train_indices: list[list[int]]\n    val_indices: list[list[int]]\n    test_indices: list[list[int]]\n    metadata: dict[str, any]\n    mini_batch_size: int | None = None\n    \"\"\"Information on how to split the data\n    this will get pass to the Dataset to know which part of the data\n    they need to access\n\n    Parameters\n    ----------\n    files : list[str]\n        List of files\n    train_indices : list[list[str]]\n        List of indices for training `train_indices[file_idx][batch_idx]` where `file_idx` must correspond\n        to the idx of `files` parameter\n    val_indices : list[list[str]]\n        List of indices for validation `val_indices[file_idx][batch_idx]` where `file_idx` must correspond\n        to the idx of `files` parameter\n    test_indices : list[list[str]]\n        List of indices for testing `test_indices[file_idx][batch_idx]` where `file_idx` must correspond\n        to the idx of `files` parameter\n    metadata : dict[str, any]\n        Data to pass on to the Dataset and model\n    mini_batch_size : int | None\n        How much data to send to the model\n    \"\"\"\n\n    def to_dict(self) -&gt; dict[str, any]:\n        return {\n            \"files\": self.files,\n            \"train_indices\": self.train_indices,\n            \"val_indices\": self.val_indices,\n            \"test_indices\": self.test_indices,\n            \"metadata\": self.metadata,\n            \"mini_batch_size\": self.mini_batch_size,\n        }\n</code></pre>"},{"location":"apis/scrna/#protoplast.scrna.anndata.strategy.SplitInfo.mini_batch_size","title":"<code>mini_batch_size: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Information on how to split the data this will get pass to the Dataset to know which part of the data they need to access</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list[str]</code> <p>List of files</p> required <code>train_indices</code> <code>list[list[str]]</code> <p>List of indices for training <code>train_indices[file_idx][batch_idx]</code> where <code>file_idx</code> must correspond to the idx of <code>files</code> parameter</p> required <code>val_indices</code> <code>list[list[str]]</code> <p>List of indices for validation <code>val_indices[file_idx][batch_idx]</code> where <code>file_idx</code> must correspond to the idx of <code>files</code> parameter</p> required <code>test_indices</code> <code>list[list[str]]</code> <p>List of indices for testing <code>test_indices[file_idx][batch_idx]</code> where <code>file_idx</code> must correspond to the idx of <code>files</code> parameter</p> required <code>metadata</code> <code>dict[str, any]</code> <p>Data to pass on to the Dataset and model</p> required <code>mini_batch_size</code> <code>int | None</code> <p>How much data to send to the model</p> required"},{"location":"apis/scrna/#protoplast.scrna.anndata.strategy.SequentialShuffleStrategy","title":"<code>protoplast.scrna.anndata.strategy.SequentialShuffleStrategy</code>","text":"<p>               Bases: <code>ShuffleStrategy</code></p> <p>Return the data in a sequential way randomness is not guarantee there is a high chance the data will come from nearby rows this might affect your training accuracy depending on how the anndata are ordered you can overcome this by preshuffling the data manually yourself if this is an issue</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>list[str]</code> <p>List of file paths</p> required <code>batch_size</code> <code>int</code> <p>How much data to fetch</p> required <code>total_workers</code> <code>int</code> <p>Total workers this is equal to number of processes times number of threads per process</p> required <code>test_size</code> <code>float | None</code> <p>Fraction of test data for example 0.1 means 10% will be split for testing, by default None</p> <code>None</code> <code>validation_size</code> <code>float | None</code> <p>Fraction of validation data for example 0.2 means 20% will be split for validation, by default None</p> <code>None</code> <code>random_seed</code> <code>int | None</code> <p>Seed to randomize the split set this to None if you want this to be completely random, by default 42</p> <code>42</code> <code>metadata_cb</code> <code>Callable[[AnnData, dict], None] | None</code> <p>Callback to mutate metadata recommended for passing data from <code>obs</code> or <code>var</code> or any additional data your models required by default cell_line_metadata_cb</p> <code>None</code> <code>is_shuffled</code> <code>bool</code> <p>Whether to shuffle the data or not this will be deprecated soon, by default True</p> <code>False</code> <code>pre_fetch_then_batch</code> <code>int | None</code> <p>The prefetch factor the total size of data fetch will be equal to <code>pre_fetch_then_batch * batch_size</code></p> <code>16</code> <code>drop_last</code> <code>bool</code> <p>If there is true drop the remainder, default to True otherwise duplicate the data to make sure the data is evenly distributed to all the workers</p> <code>True</code> Source code in <code>src/protoplast/scrna/anndata/strategy.py</code> <pre><code>class SequentialShuffleStrategy(ShuffleStrategy):\n    \"\"\"Return the data in a sequential way randomness is not guarantee\n    there is a high chance the data will come from nearby rows this might\n    affect your training accuracy depending on how the anndata are ordered you can\n    overcome this by preshuffling the data manually yourself if this is an issue\n\n    Parameters\n    ----------\n    file_paths : list[str]\n        List of file paths\n    batch_size : int\n        How much data to fetch\n    total_workers : int\n        Total workers this is equal to number of processes times number of threads per process\n    test_size : float | None, optional\n        Fraction of test data for example 0.1 means 10% will be split for testing, by default None\n    validation_size : float | None, optional\n        Fraction of validation data for example 0.2 means 20% will be split for validation, by default None\n    random_seed : int | None, optional\n        Seed to randomize the split set this to None if you want this to be completely random, by default 42\n    metadata_cb : Callable[[anndata.AnnData, dict], None] | None, optional\n        Callback to mutate metadata recommended for passing data from `obs` or `var`\n        or any additional data your models required\n        by default cell_line_metadata_cb\n    is_shuffled : bool, optional\n        Whether to shuffle the data or not this will be deprecated soon, by default True\n    pre_fetch_then_batch : int | None\n        The prefetch factor the total size of data fetch will be equal to `pre_fetch_then_batch * batch_size`\n    drop_last : bool\n        If there is true drop the remainder, default to True otherwise duplicate the data to make sure the\n        data is evenly distributed to all the workers\n    \"\"\"\n\n    def __init__(\n        self,\n        file_paths: list[str],\n        batch_size: int,\n        total_workers: int,\n        test_size: float | None = None,\n        validation_size: float | None = None,\n        random_seed: int | None = 42,\n        metadata_cb: Callable[[anndata.AnnData, dict], None] | None = None,\n        is_shuffled: bool = False,\n        pre_fetch_then_batch: int | None = 16,\n        drop_last: bool = True,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__(\n            file_paths,\n            batch_size,\n            total_workers,\n            test_size,\n            validation_size,\n            random_seed,\n            metadata_cb,\n            is_shuffled,\n        )\n        self.pre_fetch_then_batch = pre_fetch_then_batch\n        self.drop_last = drop_last\n        self.is_disable_balancing = kwargs.get(\"is_disable_balancing\", False)\n\n    def split(self) -&gt; SplitInfo:\n        split_dict = ann_split_data(\n            self.file_paths,\n            self.batch_size,\n            self.total_workers,\n            self.test_size,\n            self.validation_size,\n            self.rng,\n            self.metadata_cb,\n            self.is_shuffled,\n            self.drop_last,\n            prefetch_factor=self.pre_fetch_then_batch,\n            is_disable_balancing=self.is_disable_balancing,\n        )\n        # this will be passed to the dataset, inorder to know the mini batch size\n        if self.pre_fetch_then_batch:\n            split_dict[\"mini_batch_size\"] = self.batch_size\n        else:\n            # yield everything we read\n            split_dict[\"mini_batch_size\"] = None\n        return SplitInfo(**split_dict)\n\n    def mixer(self, batch: list):\n        return super().mixer(batch)\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":""},{"location":"tutorials/#quick-start","title":"Quick Start","text":"<p>Get up and running with minimal setup. Learn how to install dependencies, prepare your data, and run your first example in just a few commands.</p> <p>Learn more</p>"},{"location":"tutorials/#common-use-cases","title":"Common Use Cases","text":""},{"location":"tutorials/#train-a-pertubation-model","title":"Train a Pertubation Model","text":"<p>Step through the process of training a model to predict cellular responses to genetic or chemical perturbations using your own single-cell datasets.</p> <p>Learn more</p>"},{"location":"tutorials/#use-custom-models","title":"Use custom models","text":"<p>Integrate external machine learning models into the workflow. Learn how to plug in your own architectures or pretrained models for flexible experimentation.</p> <p>Learn more</p>"},{"location":"tutorials/#create-a-submission-to-the-virtual-cell-challenge","title":"Create a submission to the Virtual Cell Challenge","text":"<p>Learn how to package your trained model and datasets into the required format for the Virtual Cell Challenge. This section walks you through preparing submission files, validating them locally, and uploading your entry for official evaluation.</p> <p>Learn more</p>"},{"location":"tutorials/#more-tutorials","title":"More Tutorials","text":"<p>Please visit protoplast-ml-example to get the latest examples of PROTOplast.</p>"}]}