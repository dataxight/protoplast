{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66267ca4-71ee-44a2-b428-723c23e02526",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Showcasing Protoplast Checkpointing in Perturbation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook showcases the checkpointing feature in PROTOplast, which enables resuming model training even after interruptions or switching to a different dataset. It demonstrates how to save and load training checkpoints, making it easy to continue model development without starting from scratch. This is particularly useful for long training sessions, experimentation with various datasets, or training across multiple sessions or environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vblA",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Applied AnnDataFileManager patch\n",
      "âœ“ Applied AnnDataFileManager patch\n"
     ]
    }
   ],
   "source": [
    "import anndata\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import protoplast as pt\n",
    "import ray\n",
    "import torch\n",
    "\n",
    "from anndata.experimental import AnnCollection\n",
    "from protoplast.scrna.anndata.trainer import RayTrainRunner\n",
    "from protoplast.scrna.anndata.torch_dataloader import DistributedAnnDataset\n",
    "from protoplast.scrna.anndata.torch_dataloader import cell_line_metadata_cb\n",
    "\n",
    "from ray.train import Checkpoint\n",
    "from ray.train.lightning import RayDDPStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 2. Dataset pre-processing\n",
    "\n",
    "We begin by reading the two datasets used to train the perturbation model in this notebook. To ensure compatibility, the model requires that both datasets share a common set of features (e.g., genes).\n",
    "\n",
    "In the following section, we create a unified view by performing an **inner join** on the two datasets based on shared features. During this step, we:\n",
    "\n",
    "- Identify and record the **number of overlapping genes** (shared features),\n",
    "- Capture the **indices** of these shared genes in each dataset,\n",
    "- Extract the list of **perturbed genes** specific to each dataset,\n",
    "- And prepare **metadata** necessary for consistent training across datasets.\n",
    "\n",
    "This alignment is essential to ensure the model receives a consistent input/output structure regardless of the dataset source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_PATHS = [\"/mnt/hdd2/nam/hct116.h5ad\",\n",
    "           \"/mnt/hdd2/tan/competition_support_set/competition_train.h5\"]\n",
    "adatas = [anndata.io.read_h5ad(p, backed = \"r\") for p in DS_PATHS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, adata in enumerate(adatas):\n",
    "    if idx == 0:\n",
    "        # \n",
    "        adata.obs[\"target_gene\"] = adata.obs[[\"gene_target\"]]\n",
    "        adata.obs[\"cell_type\"] = \"HCT116\"\n",
    "    else:\n",
    "        adata.obs = adata.obs[[\"target_gene\", \"cell_type\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Xref",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of genes 18080\n"
     ]
    }
   ],
   "source": [
    "# Create a view of all dataset\n",
    "ds_view = AnnCollection(adatas, join_vars = \"inner\")\n",
    "\n",
    "# Record the genes shared by the training datasets\n",
    "n_genes = ds_view.n_vars\n",
    "genes = ds_view.var_names.tolist()\n",
    "perts = ds_view.obs[\"target_gene\"].unique().tolist()\n",
    "cell_types = ds_view.obs[\"cell_type\"].unique().tolist()\n",
    "\n",
    "print(\"Number of genes\", n_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the indices of the shared variables in the anndata object to help transform\n",
    "# yieled data batch later in training step\n",
    "shared_vars = {}\n",
    "for idx_i, adata_i in enumerate(adatas):\n",
    "    shared_vars[idx_i] = np.where(np.isin(adata_i.var_names, genes))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 3. Define model & configure training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_per_worker = 48\n",
    "test_size = 0.2 \n",
    "val_size = 0.0 # if you have only training and test data, just put val_size = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "from state.tx.models.perturb_mean import PerturbMeanPerturbationModel # import original class\n",
    "\n",
    "class PerturbMeanGlobalModel(PerturbMeanPerturbationModel):\n",
    "    def __init__(self, *wargs, **kwargs):\n",
    "        kwargs['gene_decoder_bool'] = False\n",
    "        super().__init__(*wargs, **kwargs)\n",
    "\n",
    "    \"\"\"\n",
    "    Extended class of PerturbMeanPerturbationModel where prediction ignores\n",
    "    per-cell control embedding and uses only global basal + offset.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, batch: dict) -> torch.Tensor:\n",
    "        B = len(batch[\"pert_name\"])\n",
    "        device = self.dummy_param.device\n",
    "        pred_out = torch.zeros((B, self.output_dim), device=device)\n",
    "\n",
    "        for i in range(B):\n",
    "            p_name = str(batch[\"pert_name\"][i])\n",
    "            offset_vec = self.pert_mean_offsets.get(p_name)\n",
    "            if offset_vec is None:\n",
    "                offset_vec = torch.zeros(self.output_dim, device=device)\n",
    "\n",
    "            # Use global basal instead of batch[\"ctrl_cell_emb\"]\n",
    "            pred_out[i] = self.global_basal.to(device) + offset_vec.to(device)\n",
    "\n",
    "        return pred_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kclp",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 4. Train on `HCT116_filtered_dual_guide_cells` dataset\n",
    "\n",
    "We first train this perturbation model on a dataset that contains HCT116 cells. We need to define a callback function to set up a metadata for the perturbation model based on the dataset's attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "emfo",
   "metadata": {},
   "outputs": [],
   "source": [
    "hct116_adata = anndata.read_h5ad(DS_PATHS[0], backed = \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "Hstk",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>num_features</th>\n",
       "      <th>guide_target</th>\n",
       "      <th>gene_target</th>\n",
       "      <th>n_genes_by_counts</th>\n",
       "      <th>total_counts</th>\n",
       "      <th>total_counts_mt</th>\n",
       "      <th>pct_counts_mt</th>\n",
       "      <th>pass_guide_filter</th>\n",
       "      <th>target_gene</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>batch_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAACCAAAGACATGTT-HCT116_Batch1</th>\n",
       "      <td>HCT116_Batch1</td>\n",
       "      <td>2</td>\n",
       "      <td>ST14_P1P2-1|ST14_P1P2-2</td>\n",
       "      <td>ST14</td>\n",
       "      <td>4883</td>\n",
       "      <td>19136.0</td>\n",
       "      <td>1179.0</td>\n",
       "      <td>6.161162</td>\n",
       "      <td>True</td>\n",
       "      <td>ST14</td>\n",
       "      <td>hct116</td>\n",
       "      <td>HCT116_Batch1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCAAAGACCCAAC-HCT116_Batch1</th>\n",
       "      <td>HCT116_Batch1</td>\n",
       "      <td>2</td>\n",
       "      <td>SIGLEC5_P1P2-1|SIGLEC5_P1P2-2</td>\n",
       "      <td>SIGLEC5</td>\n",
       "      <td>8130</td>\n",
       "      <td>47916.0</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>3.259871</td>\n",
       "      <td>True</td>\n",
       "      <td>SIGLEC5</td>\n",
       "      <td>hct116</td>\n",
       "      <td>HCT116_Batch1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCAAAGAGGTACG-HCT116_Batch1</th>\n",
       "      <td>HCT116_Batch1</td>\n",
       "      <td>2</td>\n",
       "      <td>VSNL1_P1P2-1|VSNL1_P1P2-2</td>\n",
       "      <td>VSNL1</td>\n",
       "      <td>6531</td>\n",
       "      <td>28435.0</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>3.664498</td>\n",
       "      <td>True</td>\n",
       "      <td>VSNL1</td>\n",
       "      <td>hct116</td>\n",
       "      <td>HCT116_Batch1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCAAAGCGATTAT-HCT116_Batch1</th>\n",
       "      <td>HCT116_Batch1</td>\n",
       "      <td>2</td>\n",
       "      <td>KCNK7_P1P2-1|KCNK7_P1P2-2</td>\n",
       "      <td>KCNK7</td>\n",
       "      <td>5931</td>\n",
       "      <td>26080.0</td>\n",
       "      <td>1087.0</td>\n",
       "      <td>4.167945</td>\n",
       "      <td>True</td>\n",
       "      <td>KCNK7</td>\n",
       "      <td>hct116</td>\n",
       "      <td>HCT116_Batch1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCAAAGGCTTAAT-HCT116_Batch1</th>\n",
       "      <td>HCT116_Batch1</td>\n",
       "      <td>2</td>\n",
       "      <td>APOA4_P1P2-1|APOA4_P1P2-2</td>\n",
       "      <td>APOA4</td>\n",
       "      <td>7157</td>\n",
       "      <td>38366.0</td>\n",
       "      <td>955.0</td>\n",
       "      <td>2.489183</td>\n",
       "      <td>True</td>\n",
       "      <td>APOA4</td>\n",
       "      <td>hct116</td>\n",
       "      <td>HCT116_Batch1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sample  num_features  \\\n",
       "AAACCAAAGACATGTT-HCT116_Batch1  HCT116_Batch1             2   \n",
       "AAACCAAAGACCCAAC-HCT116_Batch1  HCT116_Batch1             2   \n",
       "AAACCAAAGAGGTACG-HCT116_Batch1  HCT116_Batch1             2   \n",
       "AAACCAAAGCGATTAT-HCT116_Batch1  HCT116_Batch1             2   \n",
       "AAACCAAAGGCTTAAT-HCT116_Batch1  HCT116_Batch1             2   \n",
       "\n",
       "                                                 guide_target gene_target  \\\n",
       "AAACCAAAGACATGTT-HCT116_Batch1        ST14_P1P2-1|ST14_P1P2-2        ST14   \n",
       "AAACCAAAGACCCAAC-HCT116_Batch1  SIGLEC5_P1P2-1|SIGLEC5_P1P2-2     SIGLEC5   \n",
       "AAACCAAAGAGGTACG-HCT116_Batch1      VSNL1_P1P2-1|VSNL1_P1P2-2       VSNL1   \n",
       "AAACCAAAGCGATTAT-HCT116_Batch1      KCNK7_P1P2-1|KCNK7_P1P2-2       KCNK7   \n",
       "AAACCAAAGGCTTAAT-HCT116_Batch1      APOA4_P1P2-1|APOA4_P1P2-2       APOA4   \n",
       "\n",
       "                                n_genes_by_counts  total_counts  \\\n",
       "AAACCAAAGACATGTT-HCT116_Batch1               4883       19136.0   \n",
       "AAACCAAAGACCCAAC-HCT116_Batch1               8130       47916.0   \n",
       "AAACCAAAGAGGTACG-HCT116_Batch1               6531       28435.0   \n",
       "AAACCAAAGCGATTAT-HCT116_Batch1               5931       26080.0   \n",
       "AAACCAAAGGCTTAAT-HCT116_Batch1               7157       38366.0   \n",
       "\n",
       "                                total_counts_mt  pct_counts_mt  \\\n",
       "AAACCAAAGACATGTT-HCT116_Batch1           1179.0       6.161162   \n",
       "AAACCAAAGACCCAAC-HCT116_Batch1           1562.0       3.259871   \n",
       "AAACCAAAGAGGTACG-HCT116_Batch1           1042.0       3.664498   \n",
       "AAACCAAAGCGATTAT-HCT116_Batch1           1087.0       4.167945   \n",
       "AAACCAAAGGCTTAAT-HCT116_Batch1            955.0       2.489183   \n",
       "\n",
       "                                pass_guide_filter target_gene cell_type  \\\n",
       "AAACCAAAGACATGTT-HCT116_Batch1               True        ST14    hct116   \n",
       "AAACCAAAGACCCAAC-HCT116_Batch1               True     SIGLEC5    hct116   \n",
       "AAACCAAAGAGGTACG-HCT116_Batch1               True       VSNL1    hct116   \n",
       "AAACCAAAGCGATTAT-HCT116_Batch1               True       KCNK7    hct116   \n",
       "AAACCAAAGGCTTAAT-HCT116_Batch1               True       APOA4    hct116   \n",
       "\n",
       "                                    batch_var  \n",
       "AAACCAAAGACATGTT-HCT116_Batch1  HCT116_Batch1  \n",
       "AAACCAAAGACCCAAC-HCT116_Batch1  HCT116_Batch1  \n",
       "AAACCAAAGAGGTACG-HCT116_Batch1  HCT116_Batch1  \n",
       "AAACCAAAGCGATTAT-HCT116_Batch1  HCT116_Batch1  \n",
       "AAACCAAAGGCTTAAT-HCT116_Batch1  HCT116_Batch1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hct116_adata.obs.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "nWHF",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hct116_perturbmean_metadata_cb(ad: anndata.AnnData, metadata: dict):\n",
    "    metadata[\"input_dim\"] = n_genes\n",
    "    metadata[\"output_dim\"] = n_genes\n",
    "    metadata[\"hidden_dim\"] = 10\n",
    "    metadata[\"pert_dim\"] = len(perts)\n",
    "    metadata[\"lr\"] = 1e-3  \n",
    "\n",
    "    metadata[\"gene_names\"] = genes\n",
    "    metadata[\"pert_names\"] = perts\n",
    "    metadata[\"cell_types\"] = cell_types\n",
    "\n",
    "    metadata[\"control_pert\"] = \"Non-Targeting\"\n",
    "    metadata[\"embed_key\"] = \"X\"\n",
    "    metadata[\"output_space\"] = \"gene\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HCT116_PerturbAnnDataset(DistributedAnnDataset):\n",
    "    def transform(self, start: int, end: int):\n",
    "        X = super().transform(start, end)\n",
    "\n",
    "        # Subset X matrix to include only genes appear in all dataset\n",
    "        # Need to densify the data\n",
    "        X = X.to_dense()[:, shared_vars[0]]\n",
    "\n",
    "        # Metadata froms self.ad\n",
    "        pert_names = self.ad.obs[\"gene_target\"].iloc[start:end].astype(str).to_list()\n",
    "        cell_lines = [\"HCT116\"] * len(pert_names)\n",
    "\n",
    "        return {\n",
    "            \"pert_name\": pert_names,\n",
    "            \"cell_type\": cell_lines,\n",
    "            \"pert_cell_counts\": X,\n",
    "            \"pert_cell_emb\": X,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ROlb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 03:27:45,030\tINFO worker.py:1951 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainTrainable pid=198522)\u001b[0m âœ“ Applied AnnDataFileManager patch\n",
      "\u001b[36m(TrainTrainable pid=198522)\u001b[0m âœ“ Applied AnnDataFileManager patch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=198522)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=198522)\u001b[0m - (node_id=49b56875e4b5a843fc7ea0f31d82a4dd8fd27def9d910ce27d0768fd, ip=192.168.1.226, pid=198782) world_rank=0, local_rank=0, node_rank=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m âœ“ Applied AnnDataFileManager patch\n",
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m âœ“ Applied AnnDataFileManager patch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m =========Starting the training on 0 with num threads: 48=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m   | Name         | Type    | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m -------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m 0 | loss_fn      | MSELoss | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m   | other params | n/a     | 1      | n/a  \n",
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m -------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m 1         Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m 1         Total params\n",
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m 0.000     Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m 1         Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m 0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "Epoch 0:   0%|          | 0/125 [00:00<?, ?it/s]\n",
      "Epoch 0:   6%|â–Œ         | 7/125 [00:00<00:09, 12.28it/s, v_num=0, train_loss=0.0167]\n",
      "Epoch 0:  11%|â–ˆ         | 14/125 [00:00<00:05, 20.71it/s, v_num=0, train_loss=0.0169]\n",
      "Epoch 0:  12%|â–ˆâ–        | 15/125 [00:00<00:05, 21.75it/s, v_num=0, train_loss=0.0117]\n",
      "Epoch 0:  18%|â–ˆâ–Š        | 23/125 [00:00<00:03, 28.44it/s, v_num=0, train_loss=0.0155]\n",
      "Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 29/125 [00:00<00:03, 31.73it/s, v_num=0, train_loss=0.0195]\n",
      "Epoch 0:  24%|â–ˆâ–ˆâ–       | 30/125 [00:00<00:02, 32.32it/s, v_num=0, train_loss=0.0136]\n",
      "Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 37/125 [00:01<00:02, 35.40it/s, v_num=0, train_loss=0.0158]\n",
      "Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 43/125 [00:01<00:02, 37.53it/s, v_num=0, train_loss=0.0162]\n",
      "Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 44/125 [00:01<00:02, 37.84it/s, v_num=0, train_loss=0.0124]\n",
      "Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 51/125 [00:01<00:01, 40.15it/s, v_num=0, train_loss=0.0231]\n",
      "Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 52/125 [00:01<00:01, 40.51it/s, v_num=0, train_loss=0.021] \n",
      "Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 60/125 [00:01<00:01, 43.11it/s, v_num=0, train_loss=0.0219]\n",
      "Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 68/125 [00:01<00:01, 45.31it/s, v_num=0, train_loss=0.0144]\n",
      "Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 75/125 [00:01<00:01, 46.90it/s, v_num=0, train_loss=0.0215]\n",
      "Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 76/125 [00:01<00:01, 46.69it/s, v_num=0, train_loss=0.0152]\n",
      "Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 83/125 [00:01<00:00, 47.90it/s, v_num=0, train_loss=0.0159]\n",
      "Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 84/125 [00:01<00:00, 48.10it/s, v_num=0, train_loss=0.0225]\n",
      "Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 92/125 [00:01<00:00, 49.62it/s, v_num=0, train_loss=0.0194]\n",
      "Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 93/125 [00:01<00:00, 49.81it/s, v_num=0, train_loss=0.0174]\n",
      "Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 94/125 [00:01<00:00, 49.96it/s, v_num=0, train_loss=0.0155]\n",
      "Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 101/125 [00:01<00:00, 50.94it/s, v_num=0, train_loss=0.0194]\n",
      "Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 102/125 [00:01<00:00, 51.10it/s, v_num=0, train_loss=0.0173]\n",
      "Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 110/125 [00:02<00:00, 52.12it/s, v_num=0, train_loss=0.0149]\n",
      "Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 118/125 [00:02<00:00, 53.12it/s, v_num=0, train_loss=0.0193]\n",
      "Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 119/125 [00:02<00:00, 53.28it/s, v_num=0, train_loss=0.0187]\n",
      "Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 120/125 [00:02<00:00, 53.42it/s, v_num=0, train_loss=0.0161]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:02<00:00, 54.10it/s, v_num=0, train_loss=0.0169]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/nam/protoplast_results/TorchTrainer_2025-09-19_03-27-49/TorchTrainer_9e62d_00000_0_2025-09-19_03-27-49/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:09<00:00, 13.39it/s, v_num=0, train_loss=0.0169]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m `Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:14<00:00,  8.49it/s, v_num=0, train_loss=0.0169]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=198782)\u001b[0m [rank0]:[W919 03:28:40.437343211 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "# Set up training\n",
    "PerturbMeanPerturbationModel_trainer = RayTrainRunner(\n",
    "    PerturbMeanGlobalModel,\n",
    "    HCT116_PerturbAnnDataset,\n",
    "    [\"input_dim\",\n",
    "    \"output_dim\",\n",
    "    \"hidden_dim\",      \n",
    "    \"pert_dim\",        \n",
    "    \"lr\",\n",
    "    \"control_pert\",    # \"Non-Tageting\"\n",
    "    \"embed_key\",       \n",
    "    \"output_space\",    # \"gene\"\n",
    "    ],\n",
    "    metadata_cb = hct116_perturbmean_metadata_cb,\n",
    "    sparse_keys = \"X\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = PerturbMeanPerturbationModel_trainer.train([DS_PATHS[0]],\n",
    "                                                    batch_size = 64,\n",
    "                                                    test_size = test_size, \n",
    "                                                    val_size = val_size,\n",
    "                                                    num_workers = 1,\n",
    "                                                    resource_per_worker = {\"GPU\": 1, \"CPU\": thread_per_worker})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TqIu",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 5. Train on `competition_train` dataset\n",
    "\n",
    "We now have a checkpoint saved after training the perturbation model using the first dataset. To continue training on a different dataset, several adjustments are necessary to ensure compatibility and correct model behavior.\n",
    "\n",
    "- We need to define a new **metadata callback function** that sets up the appropriate configurations for the model when training under the new dataset.\n",
    "\n",
    "Since the second dataset may have different **input dimensions** or **metadata fields**, we also define a custom `AnnDataset` class. This class is responsible for transforming each training batch accordingly, ensuring:\n",
    "\n",
    "- Features are mapped to the expected input space,\n",
    "- Metadata is correctly aligned with the model's expectations,\n",
    "- Any dataset-specific preprocessing is applied consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_adata = anndata.read_h5ad(DS_PATHS[1], backed = \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "DnEU",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_gene</th>\n",
       "      <th>guide_id</th>\n",
       "      <th>batch</th>\n",
       "      <th>batch_var</th>\n",
       "      <th>cell_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAACAAGCAACCTTGTACTTTAGG-Flex_1_01</th>\n",
       "      <td>CHMP3</td>\n",
       "      <td>CHMP3_P1P2_A|CHMP3_P1P2_B</td>\n",
       "      <td>Flex_1_01</td>\n",
       "      <td>Flex_1_01</td>\n",
       "      <td>ARC_H1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACAAGCATTGCCGCACTTTAGG-Flex_1_01</th>\n",
       "      <td>AKT2</td>\n",
       "      <td>AKT2_P1P2_A|AKT2_P1P2_B</td>\n",
       "      <td>Flex_1_01</td>\n",
       "      <td>Flex_1_01</td>\n",
       "      <td>ARC_H1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCAATCAATGTTCACTTTAGG-Flex_1_01</th>\n",
       "      <td>SHPRH</td>\n",
       "      <td>SHPRH_P1P2_A|SHPRH_P1P2_B</td>\n",
       "      <td>Flex_1_01</td>\n",
       "      <td>Flex_1_01</td>\n",
       "      <td>ARC_H1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCAATCCCTCGCTACTTTAGG-Flex_1_01</th>\n",
       "      <td>TMSB4X</td>\n",
       "      <td>TMSB4X_P1_A|TMSB4X_P1_B</td>\n",
       "      <td>Flex_1_01</td>\n",
       "      <td>Flex_1_01</td>\n",
       "      <td>ARC_H1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCAATCTAAATCCACTTTAGG-Flex_1_01</th>\n",
       "      <td>KLF10</td>\n",
       "      <td>KLF10_P2_A|KLF10_P2_B</td>\n",
       "      <td>Flex_1_01</td>\n",
       "      <td>Flex_1_01</td>\n",
       "      <td>ARC_H1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   target_gene                   guide_id  \\\n",
       "AAACAAGCAACCTTGTACTTTAGG-Flex_1_01       CHMP3  CHMP3_P1P2_A|CHMP3_P1P2_B   \n",
       "AAACAAGCATTGCCGCACTTTAGG-Flex_1_01        AKT2    AKT2_P1P2_A|AKT2_P1P2_B   \n",
       "AAACCAATCAATGTTCACTTTAGG-Flex_1_01       SHPRH  SHPRH_P1P2_A|SHPRH_P1P2_B   \n",
       "AAACCAATCCCTCGCTACTTTAGG-Flex_1_01      TMSB4X    TMSB4X_P1_A|TMSB4X_P1_B   \n",
       "AAACCAATCTAAATCCACTTTAGG-Flex_1_01       KLF10      KLF10_P2_A|KLF10_P2_B   \n",
       "\n",
       "                                        batch  batch_var cell_type  \n",
       "AAACAAGCAACCTTGTACTTTAGG-Flex_1_01  Flex_1_01  Flex_1_01    ARC_H1  \n",
       "AAACAAGCATTGCCGCACTTTAGG-Flex_1_01  Flex_1_01  Flex_1_01    ARC_H1  \n",
       "AAACCAATCAATGTTCACTTTAGG-Flex_1_01  Flex_1_01  Flex_1_01    ARC_H1  \n",
       "AAACCAATCCCTCGCTACTTTAGG-Flex_1_01  Flex_1_01  Flex_1_01    ARC_H1  \n",
       "AAACCAATCTAAATCCACTTTAGG-Flex_1_01  Flex_1_01  Flex_1_01    ARC_H1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competition_adata.obs.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ulZA",
   "metadata": {},
   "outputs": [],
   "source": [
    "def competition_perturbmean_metadata_cb(ad: anndata.AnnData, metadata: dict):\n",
    "    metadata[\"input_dim\"] = n_genes\n",
    "    metadata[\"output_dim\"] = n_genes\n",
    "    metadata[\"hidden_dim\"] = 10\n",
    "    metadata[\"pert_dim\"] = len(perts)\n",
    "    metadata[\"lr\"] = 1e-3\n",
    "\n",
    "    metadata[\"gene_names\"] = genes\n",
    "    metadata[\"pert_names\"] = perts\n",
    "    metadata[\"cell_types\"] = cell_types\n",
    "\n",
    "    metadata[\"control_pert\"] = \"non-targeting\"\n",
    "    metadata[\"embed_key\"] = \"X\"\n",
    "    metadata[\"output_space\"] = \"gene\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecfG",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Competition_PerturbAnnDataset(DistributedAnnDataset):\n",
    "    def transform(self, start: int, end: int):\n",
    "        X = super().transform(start, end)\n",
    "\n",
    "        # Subset X matrix to include only genes appear in all dataset\n",
    "        # Need to densify the data\n",
    "        X = X.to_dense()[:, shared_vars[1]]\n",
    "\n",
    "        # Metadata froms self.ad\n",
    "        pert_names = self.ad.obs[\"target_gene\"].iloc[start:end].astype(str).to_list()\n",
    "        cell_lines = self.ad.obs[\"cell_type\"].iloc[start:end].astype(str).to_list()\n",
    "\n",
    "        return {\n",
    "            \"pert_name\": pert_names,\n",
    "            \"cell_type\": cell_lines,\n",
    "            \"pert_cell_counts\": X,\n",
    "            \"pert_cell_emb\": X,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 03:29:20,289\tINFO worker.py:1951 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainTrainable pid=206826)\u001b[0m âœ“ Applied AnnDataFileManager patch\n",
      "\u001b[36m(TrainTrainable pid=206826)\u001b[0m âœ“ Applied AnnDataFileManager patch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=206826)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=206826)\u001b[0m - (node_id=56af46918d2afaab8fe704d215fd39de3de026f40333c93c60883644, ip=192.168.1.226, pid=207034) world_rank=0, local_rank=0, node_rank=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m âœ“ Applied AnnDataFileManager patch\n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m âœ“ Applied AnnDataFileManager patch\n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m =========Starting the training on 0 with num threads: 48=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m Restoring states from the checkpoint path at /home/nam/protoplast_results/TorchTrainer_2025-09-19_03-27-49/TorchTrainer_9e62d_00000_0_2025-09-19_03-27-49/checkpoint_000000/checkpoint.ckpt\n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m   | Name         | Type    | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m -------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m 0 | loss_fn      | MSELoss | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m   | other params | n/a     | 1      | n/a  \n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m -------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m 1         Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m 1         Total params\n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m 0.000     Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m 1         Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m 0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m Restored all states from the checkpoint at /home/nam/protoplast_results/TorchTrainer_2025-09-19_03-27-49/TorchTrainer_9e62d_00000_0_2025-09-19_03-27-49/checkpoint_000000/checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/109 [00:00<?, ?it/s]\n",
      "Epoch 1:   1%|          | 1/109 [00:07<14:13,  0.13it/s, v_num=0, train_loss=0.184]\n",
      "Epoch 1:   2%|â–         | 2/109 [00:08<07:35,  0.23it/s, v_num=0, train_loss=0.173]\n",
      "Epoch 1:   3%|â–Ž         | 3/109 [00:09<05:20,  0.33it/s, v_num=0, train_loss=0.187]\n",
      "Epoch 1:   4%|â–Ž         | 4/109 [00:09<04:14,  0.41it/s, v_num=0, train_loss=0.177]\n",
      "Epoch 1:   5%|â–         | 5/109 [00:10<03:32,  0.49it/s, v_num=0, train_loss=0.184]\n",
      "Epoch 1:   6%|â–Œ         | 6/109 [00:10<03:04,  0.56it/s, v_num=0, train_loss=0.175]\n",
      "Epoch 1:   6%|â–‹         | 7/109 [00:11<02:43,  0.62it/s, v_num=0, train_loss=0.187]\n",
      "Epoch 1:   7%|â–‹         | 8/109 [00:11<02:29,  0.68it/s, v_num=0, train_loss=0.178]\n",
      "Epoch 1:   8%|â–Š         | 9/109 [00:12<02:17,  0.73it/s, v_num=0, train_loss=0.185]\n",
      "Epoch 1:   9%|â–‰         | 10/109 [00:12<02:07,  0.78it/s, v_num=0, train_loss=0.175]\n",
      "Epoch 1:  10%|â–ˆ         | 11/109 [00:13<01:59,  0.82it/s, v_num=0, train_loss=0.178]\n",
      "Epoch 1:  11%|â–ˆ         | 12/109 [00:13<01:52,  0.86it/s, v_num=0, train_loss=0.180]\n",
      "Epoch 1:  12%|â–ˆâ–        | 13/109 [00:14<01:46,  0.90it/s, v_num=0, train_loss=0.180]\n",
      "Epoch 1:  13%|â–ˆâ–Ž        | 14/109 [00:14<01:41,  0.94it/s, v_num=0, train_loss=0.173]\n",
      "Epoch 1:  14%|â–ˆâ–        | 15/109 [00:15<01:36,  0.97it/s, v_num=0, train_loss=0.172]\n",
      "Epoch 1:  15%|â–ˆâ–        | 16/109 [00:15<01:32,  1.01it/s, v_num=0, train_loss=0.184]\n",
      "Epoch 1:  16%|â–ˆâ–Œ        | 17/109 [00:16<01:28,  1.04it/s, v_num=0, train_loss=0.179]\n",
      "Epoch 1:  17%|â–ˆâ–‹        | 18/109 [00:16<01:24,  1.08it/s, v_num=0, train_loss=0.178]\n",
      "Epoch 1:  17%|â–ˆâ–‹        | 19/109 [00:16<01:19,  1.13it/s, v_num=0, train_loss=0.176]\n",
      "Epoch 1:  18%|â–ˆâ–Š        | 20/109 [00:17<01:16,  1.17it/s, v_num=0, train_loss=0.178]\n",
      "Epoch 1:  19%|â–ˆâ–‰        | 21/109 [00:17<01:14,  1.17it/s, v_num=0, train_loss=0.176]\n",
      "Epoch 1:  20%|â–ˆâ–ˆ        | 22/109 [00:18<01:11,  1.21it/s, v_num=0, train_loss=0.172]\n",
      "Epoch 1:  21%|â–ˆâ–ˆ        | 23/109 [00:18<01:08,  1.25it/s, v_num=0, train_loss=0.182]\n",
      "Epoch 1:  22%|â–ˆâ–ˆâ–       | 24/109 [00:18<01:06,  1.28it/s, v_num=0, train_loss=0.176]\n",
      "Epoch 1:  23%|â–ˆâ–ˆâ–Ž       | 25/109 [00:19<01:06,  1.26it/s, v_num=0, train_loss=0.174]\n",
      "Epoch 1:  24%|â–ˆâ–ˆâ–       | 26/109 [00:20<01:04,  1.29it/s, v_num=0, train_loss=0.176]\n",
      "Epoch 1:  25%|â–ˆâ–ˆâ–       | 27/109 [00:20<01:02,  1.31it/s, v_num=0, train_loss=0.181]\n",
      "Epoch 1:  26%|â–ˆâ–ˆâ–Œ       | 28/109 [00:21<01:01,  1.33it/s, v_num=0, train_loss=0.173]\n",
      "Epoch 1:  27%|â–ˆâ–ˆâ–‹       | 29/109 [00:21<01:00,  1.32it/s, v_num=0, train_loss=0.175]\n",
      "Epoch 1:  28%|â–ˆâ–ˆâ–Š       | 30/109 [00:22<00:59,  1.33it/s, v_num=0, train_loss=0.179]\n",
      "Epoch 1:  28%|â–ˆâ–ˆâ–Š       | 31/109 [00:22<00:57,  1.35it/s, v_num=0, train_loss=0.177]\n",
      "Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 32/109 [00:23<00:56,  1.36it/s, v_num=0, train_loss=0.174]\n",
      "Epoch 1:  30%|â–ˆâ–ˆâ–ˆ       | 33/109 [00:24<00:55,  1.37it/s, v_num=0, train_loss=0.172]\n",
      "Epoch 1:  31%|â–ˆâ–ˆâ–ˆ       | 34/109 [00:24<00:54,  1.39it/s, v_num=0, train_loss=0.179]\n",
      "Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 35/109 [00:25<00:53,  1.40it/s, v_num=0, train_loss=0.182]\n",
      "Epoch 1:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 36/109 [00:25<00:51,  1.40it/s, v_num=0, train_loss=0.176]\n",
      "Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 37/109 [00:26<00:50,  1.41it/s, v_num=0, train_loss=0.176]\n",
      "Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–      | 38/109 [00:26<00:49,  1.43it/s, v_num=0, train_loss=0.178]\n",
      "Epoch 1:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 39/109 [00:27<00:48,  1.44it/s, v_num=0, train_loss=0.178]\n",
      "Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 40/109 [00:27<00:47,  1.44it/s, v_num=0, train_loss=0.174]\n",
      "Epoch 1:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 41/109 [00:28<00:46,  1.45it/s, v_num=0, train_loss=0.174]\n",
      "Epoch 1:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 42/109 [00:28<00:45,  1.46it/s, v_num=0, train_loss=0.180]\n",
      "Epoch 1:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 43/109 [00:28<00:44,  1.49it/s, v_num=0, train_loss=0.175]\n",
      "Epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 44/109 [00:29<00:43,  1.51it/s, v_num=0, train_loss=0.176]\n",
      "Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 45/109 [00:29<00:41,  1.53it/s, v_num=0, train_loss=0.177]\n",
      "Epoch 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 46/109 [00:29<00:40,  1.55it/s, v_num=0, train_loss=0.181]\n",
      "Epoch 1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 47/109 [00:29<00:39,  1.57it/s, v_num=0, train_loss=0.175]\n",
      "Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 48/109 [00:31<00:39,  1.54it/s, v_num=0, train_loss=0.173]\n",
      "Epoch 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 49/109 [00:31<00:38,  1.56it/s, v_num=0, train_loss=0.176]\n",
      "Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 50/109 [00:31<00:37,  1.58it/s, v_num=0, train_loss=0.182]\n",
      "Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 51/109 [00:31<00:35,  1.61it/s, v_num=0, train_loss=0.166]\n",
      "Epoch 1:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 52/109 [00:31<00:34,  1.63it/s, v_num=0, train_loss=0.174]\n",
      "Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 53/109 [00:33<00:35,  1.59it/s, v_num=0, train_loss=0.177]\n",
      "Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 54/109 [00:33<00:34,  1.59it/s, v_num=0, train_loss=0.183]\n",
      "Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 55/109 [00:34<00:33,  1.60it/s, v_num=0, train_loss=0.183]\n",
      "Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 55/109 [00:34<00:33,  1.60it/s, v_num=0, train_loss=0.175]\n",
      "Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 56/109 [00:35<00:33,  1.59it/s, v_num=0, train_loss=0.176]\n",
      "Epoch 1:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 57/109 [00:35<00:32,  1.60it/s, v_num=0, train_loss=0.182]\n",
      "Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 58/109 [00:36<00:31,  1.60it/s, v_num=0, train_loss=0.178]\n",
      "Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 59/109 [00:37<00:31,  1.59it/s, v_num=0, train_loss=0.177]\n",
      "Epoch 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 60/109 [00:37<00:30,  1.60it/s, v_num=0, train_loss=0.180]\n",
      "Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 61/109 [00:38<00:29,  1.60it/s, v_num=0, train_loss=0.174]\n",
      "Epoch 1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 62/109 [00:38<00:29,  1.59it/s, v_num=0, train_loss=0.178]\n",
      "Epoch 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 63/109 [00:39<00:28,  1.60it/s, v_num=0, train_loss=0.178]\n",
      "Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 64/109 [00:39<00:28,  1.60it/s, v_num=0, train_loss=0.174]\n",
      "Epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 65/109 [00:40<00:27,  1.60it/s, v_num=0, train_loss=0.179]\n",
      "Epoch 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 66/109 [00:40<00:26,  1.61it/s, v_num=0, train_loss=0.179]\n",
      "Epoch 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 67/109 [00:41<00:25,  1.62it/s, v_num=0, train_loss=0.175]\n",
      "Epoch 1:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 68/109 [00:42<00:25,  1.60it/s, v_num=0, train_loss=0.178]\n",
      "Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 69/109 [00:42<00:24,  1.62it/s, v_num=0, train_loss=0.176]\n",
      "Epoch 1:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 70/109 [00:42<00:23,  1.63it/s, v_num=0, train_loss=0.178]\n",
      "Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 71/109 [00:44<00:23,  1.60it/s, v_num=0, train_loss=0.178]\n",
      "Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 72/109 [00:45<00:23,  1.60it/s, v_num=0, train_loss=0.177]\n",
      "Epoch 1:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 73/109 [00:45<00:22,  1.60it/s, v_num=0, train_loss=0.176]\n",
      "Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 74/109 [00:46<00:21,  1.60it/s, v_num=0, train_loss=0.175]\n",
      "Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 75/109 [00:46<00:21,  1.60it/s, v_num=0, train_loss=0.177]\n",
      "Epoch 1:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 76/109 [00:47<00:20,  1.60it/s, v_num=0, train_loss=0.175]\n",
      "Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 77/109 [00:48<00:20,  1.60it/s, v_num=0, train_loss=0.178]\n",
      "Epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 78/109 [00:48<00:19,  1.60it/s, v_num=0, train_loss=0.179]\n",
      "Epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 79/109 [00:49<00:18,  1.60it/s, v_num=0, train_loss=0.171]\n",
      "Epoch 1:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 80/109 [00:50<00:18,  1.60it/s, v_num=0, train_loss=0.174]\n",
      "Epoch 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 81/109 [00:50<00:17,  1.60it/s, v_num=0, train_loss=0.181]\n",
      "Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 82/109 [00:51<00:16,  1.60it/s, v_num=0, train_loss=0.175]\n",
      "Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 83/109 [00:51<00:16,  1.60it/s, v_num=0, train_loss=0.173]\n",
      "Epoch 1:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 84/109 [00:52<00:15,  1.61it/s, v_num=0, train_loss=0.182]\n",
      "Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 85/109 [00:52<00:14,  1.62it/s, v_num=0, train_loss=0.176]\n",
      "Epoch 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 86/109 [00:53<00:14,  1.61it/s, v_num=0, train_loss=0.173]\n",
      "Epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 87/109 [00:53<00:13,  1.62it/s, v_num=0, train_loss=0.179]\n",
      "Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 88/109 [00:54<00:12,  1.63it/s, v_num=0, train_loss=0.173]\n",
      "Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 89/109 [00:55<00:12,  1.61it/s, v_num=0, train_loss=0.176]\n",
      "Epoch 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 90/109 [00:55<00:11,  1.61it/s, v_num=0, train_loss=0.182]\n",
      "Epoch 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 91/109 [00:56<00:11,  1.62it/s, v_num=0, train_loss=0.177]\n",
      "Epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 92/109 [00:57<00:10,  1.60it/s, v_num=0, train_loss=0.175]\n",
      "Epoch 1:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 93/109 [00:58<00:09,  1.60it/s, v_num=0, train_loss=0.186]\n",
      "Epoch 1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 94/109 [00:58<00:09,  1.60it/s, v_num=0, train_loss=0.177]\n",
      "Epoch 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 95/109 [00:59<00:08,  1.60it/s, v_num=0, train_loss=0.175]\n",
      "Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 96/109 [00:59<00:08,  1.60it/s, v_num=0, train_loss=0.190]\n",
      "Epoch 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 97/109 [01:00<00:07,  1.61it/s, v_num=0, train_loss=0.176]\n",
      "Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 98/109 [01:01<00:06,  1.60it/s, v_num=0, train_loss=0.179]\n",
      "Epoch 1:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 99/109 [01:01<00:06,  1.60it/s, v_num=0, train_loss=0.189]\n",
      "Epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 100/109 [01:02<00:05,  1.60it/s, v_num=0, train_loss=0.175]\n",
      "Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 101/109 [01:03<00:05,  1.60it/s, v_num=0, train_loss=0.179]\n",
      "Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 102/109 [01:03<00:04,  1.60it/s, v_num=0, train_loss=0.177]\n",
      "Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 103/109 [01:04<00:03,  1.61it/s, v_num=0, train_loss=0.176]\n",
      "Epoch 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 104/109 [01:04<00:03,  1.61it/s, v_num=0, train_loss=0.177]\n",
      "Epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 105/109 [01:05<00:02,  1.61it/s, v_num=0, train_loss=0.178]\n",
      "Epoch 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 106/109 [01:05<00:01,  1.62it/s, v_num=0, train_loss=0.172]\n",
      "Epoch 1:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 107/109 [01:06<00:01,  1.61it/s, v_num=0, train_loss=0.178]\n",
      "Epoch 1:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 108/109 [01:06<00:00,  1.61it/s, v_num=0, train_loss=0.181]\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 109/109 [01:07<00:00,  1.62it/s, v_num=0, train_loss=0.175]\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 109/109 [01:07<00:00,  1.61it/s, v_num=0, train_loss=0.175]\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 109/109 [01:07<00:00,  1.61it/s, v_num=0, train_loss=0.175]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/nam/protoplast_results/TorchTrainer_2025-09-19_03-29-26/TorchTrainer_d7c14_00000_0_2025-09-19_03-29-26/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m `Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "\u001b[36m(RayTrainWorker pid=207034)\u001b[0m [rank0]:[W919 03:32:27.750894371 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "# Set up training\n",
    "competition_trainer = RayTrainRunner(\n",
    "    PerturbMeanGlobalModel,\n",
    "    Competition_PerturbAnnDataset,\n",
    "    [\"input_dim\",\n",
    "    \"output_dim\",\n",
    "    \"hidden_dim\",      \n",
    "    \"pert_dim\",\n",
    "    \"lr\",\n",
    "    \"control_pert\",    # \"non-targeting\"\n",
    "    \"embed_key\",       \n",
    "    \"output_space\",    # \"gene\"\n",
    "    ],\n",
    "    metadata_cb = competition_perturbmean_metadata_cb,\n",
    "    sparse_keys = \"X\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZBYS",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = os.path.join(result.checkpoint.path, \"checkpoint.ckpt\")\n",
    "\n",
    "competition_trainer.train([DS_PATHS[1]],\n",
    "                          max_epochs = 2,\n",
    "                          batch_size = 2048, \n",
    "                          test_size = test_size, \n",
    "                          val_size = val_size,\n",
    "                          num_workers = 1,\n",
    "                          resource_per_worker = {\"GPU\": 1, \"CPU\": thread_per_worker},\n",
    "                          ckpt_path = ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f424dd0-f4f3-40cb-bbdc-dcd9f7651852",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This brings us to the end of the tutorial notebook.\n",
    "\n",
    "This workflow highlights using checkpointing in **PROTOplast**, enabling efficient model development across diverse datasets.\n",
    "\n",
    "Feel free to explore and extend this notebook to suit your own data and use cases!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
