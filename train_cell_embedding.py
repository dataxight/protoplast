# -*- coding: utf-8 -*-
"""VCC - Build a cell embedding to classify target gene

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WtFbyjrhMd3PzaojP85i6te17EDvu1OP
"""

# !pip install scvi-colab

# from scvi_colab import install

# default
# install()

import os
import tempfile

import scanpy as sc
import scvi
import seaborn as sns
import torch

# Commented out IPython magic to ensure Python compatibility.
# %pip install requests tqdm

import requests
from tqdm.auto import tqdm  # picks the best bar for the environment

url = "https://storage.googleapis.com/vcc_data_prod/datasets/state/competition_support_set.zip"
output_path = "competition_support_set.zip"

# stream the download so we can track progress
response = requests.get(url, stream=True)
total = int(response.headers.get("content-length", 0))

with open(output_path, "wb") as f, tqdm(
    total=total, unit='B', unit_scale=True, desc="Downloading"
) as bar:
    for chunk in response.iter_content(chunk_size=8192):
        if not chunk:
            break
        f.write(chunk)
        bar.update(len(chunk))

from zipfile import ZipFile
from tqdm.auto import tqdm
import os

out_dir  = "competition_support_set"
output_path = "competition_support_set.zip"

os.makedirs(out_dir, exist_ok=True)

with ZipFile(output_path, 'r') as z:
    for member in tqdm(z.infolist(), desc="Unzipping", unit="file"):
        z.extract(member, out_dir)

adata = sc.read_h5ad("competition_support_set/competition_train.h5")

adata

import warnings

warnings.filterwarnings("ignore", category=DeprecationWarning, module="jupyter_client.session")

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import scanpy as sc
import pandas as pd
from sklearn.model_selection import train_test_split
from scvi.dataloaders import AnnDataLoader

# ---------------------------
# 0) Inputs you set
# ---------------------------
target_col = "target_gene"   # <- replace with your obs column name
hvg_n = 2000              # number of highly variable genes to keep


# ---------------------------
# 1) AnnData preprocessing
# ---------------------------
# Assumes `adata` already exists in your session
# Ensure counts are in adata.X (sparse OK), and target labels are categorical codes
assert target_col in adata.obs.columns, f"{target_col} not found in adata.obs"
print("Find hvg")
sc.pp.highly_variable_genes(adata, n_top_genes=hvg_n, flavor="seurat_v3")
print("Done finding hvg")
adata = adata[:, adata.var["highly_variable"].values].copy()
adata.var["gene_id"] = np.arange(adata.n_vars)  # integer ids for tokens

# Encode labels
labels_cat = pd.Categorical(adata.obs[target_col])
y = torch.tensor(labels_cat.codes, dtype=torch.long)
num_classes = len(labels_cat.categories)

# Train/val/test split on cell indices
idx = np.arange(adata.n_obs)
train_idx, test_idx = train_test_split(idx, test_size=0.1, random_state=0, stratify=y)
train_idx, val_idx  = train_test_split(train_idx, test_size=0.1, random_state=0, stratify=y[train_idx])

# ---------------------------
# 2) scvi AnnDataLoaders
# ---------------------------
# We ask AnnDataLoader to give us X (features) and y (labels from obs)
# `data_and_attributes` maps what to pull: ('attr_name', 'attr_key')
from scvi.data import AnnDataManager
from scvi.data.fields import LayerField, CategoricalObsField

adata_manager = AnnDataManager(fields=[LayerField("X", layer=None), CategoricalObsField(registry_key="y",attr_key=target_col)])
adata_manager.register_fields(adata)
batch_size = 16384
topk_tokens = 256         # tokens per cell (top-k expressed genes after HVG)
batch_size = 128


dl_train = AnnDataLoader(
    adata_manager,
    indices=train_idx,
    shuffle=True,
    batch_size=batch_size,
)
dl_val = AnnDataLoader(
    adata_manager,
    indices=val_idx,
    shuffle=False,
    batch_size=batch_size,
)
dl_test = AnnDataLoader(
    adata_manager,
    indices=test_idx,
    shuffle=False,
    batch_size=batch_size,
)

d_model = 128
n_heads = 8
n_layers = 4
dropout = 0.1
lr = 3e-4
max_epochs = 40
device = "cuda" if torch.cuda.is_available() else "cpu"

from models.cell_emb import ExprTransformer, token_loader, make_token_batch, to_dense

num_genes = adata.n_vars
model = ExprTransformer(
    num_genes=num_genes,
    num_classes=num_classes,
    d_model=d_model,
    n_heads=n_heads,
    n_layers=n_layers,
    dropout=dropout,
    topk_tokens=topk_tokens
).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)

def run_epoch(model, loader_iter, train=True):
    model.train(train)
    total, correct, loss_sum = 0, 0, 0.0
    for batch in loader_iter:
        tok_ids = batch["tok_ids"].to(device)
        tok_vals = batch["tok_vals"].to(device)
        yb = batch["y"].squeeze(-1).to(device, dtype=torch.long)

        logits = model(tok_ids, tok_vals)
        loss = criterion(logits, yb)

        if train:
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

        with torch.no_grad():
            preds = logits.argmax(dim=1)
            correct += (preds == yb).sum().item()
            total += yb.numel()
            loss_sum += loss.item() * yb.numel()

    acc = correct / max(total, 1)
    avg_loss = loss_sum / max(total, 1)
    return avg_loss, acc

best_val = -1.0
for epoch in range(1, max_epochs + 1):
    train_stats = run_epoch(model, token_loader(dl_train), train=True)
    val_stats   = run_epoch(model, token_loader(dl_val), train=False)
    print(f"Epoch {epoch:02d} | train loss {train_stats[0]:.4f} acc {train_stats[1]:.4f} "
          f"| val loss {val_stats[0]:.4f} acc {val_stats[1]:.4f}")

    if val_stats[1] > best_val:
        best_val = val_stats[1]
        torch.save(model.state_dict(), "best_expr_transformer.pt")

# ---------------------------
# 6) Test evaluation
# ---------------------------
model.load_state_dict(torch.load("best_expr_transformer.pt", map_location=device))
test_loss, test_acc = run_epoch(model, token_loader(dl_test), train=False)
print(f"TEST: loss {test_loss:.4f} | acc {test_acc:.4f}")